{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0a790-3299-41b6-87b4-b191fd3efe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 : Standard Library Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import yfinance as yf\n",
    "import keras_tuner as kt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import ta\n",
    "from pandas.tseries.offsets import BDay\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import pandas_market_calendars as mcal\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Input, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, \n",
    "                             mean_absolute_percentage_error, r2_score)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import skew, kurtosis, shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67173a79-1f45-43c7-9ec7-82dddc594b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Fetch the Stock Data (Time-series Only)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize dictionaries to store dataframes\n",
    "daily_data_dict = {}\n",
    "\n",
    "# List of stocks to fetch data for\n",
    "stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n",
    "\n",
    "# Define the time frames for data\n",
    "end_date = datetime.now()\n",
    "start_date_daily = end_date - timedelta(days=10*365)   # 10 years of daily data\n",
    "\n",
    "# Create directories for the data\n",
    "os.makedirs('../data/stock_data', exist_ok=True)\n",
    "\n",
    "# Function to fetch stock data\n",
    "def fetch_stock_data(ticker, start, end, interval):\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start, end=end, interval=interval)\n",
    "        if data.empty:\n",
    "            logging.warning(f\"No data retrieved for {ticker} from {start} to {end} with interval {interval}\")\n",
    "        return data.drop(columns=['Adj Close'], errors='ignore')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch and save daily time-series data\n",
    "for stock in tqdm(stocks, desc=\"Fetching stocks data\"):\n",
    "\n",
    "    # Daily Data (10 years)\n",
    "    daily_data = fetch_stock_data(stock, start_date_daily, end_date, '1d')\n",
    "    if not daily_data.empty:\n",
    "        daily_data_dict[stock] = daily_data\n",
    "        daily_data.to_csv(f'../data/stock_data/{stock}_daily.csv', index=True)\n",
    "\n",
    "    # Add a delay to avoid API rate limits\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"Time-series data fetching and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb5640-0f73-4230-bff8-7416550f8dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Feature Engineering\n",
    "\n",
    "def add_close_price_features(df):\n",
    "    # Ensure the 'Close' column is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN 'Close' values\n",
    "    df.dropna(subset=['Close'], inplace=True)\n",
    "    \n",
    "    # Sort by 'Date' if not already sorted\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Moving Averages with min_periods\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5, min_periods=1).mean()\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10, min_periods=1).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20, min_periods=1).mean()\n",
    "    df['EMA_5'] = df['Close'].ewm(span=5, adjust=False, min_periods=1).mean()\n",
    "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False, min_periods=1).mean()\n",
    "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False, min_periods=1).mean()\n",
    "    \n",
    "    # Momentum Indicators\n",
    "    df['Momentum_5'] = df['Close'] - df['Close'].shift(5)\n",
    "    df['Momentum_10'] = df['Close'] - df['Close'].shift(10)\n",
    "    df['ROC_5'] = df['Close'].pct_change(periods=5)\n",
    "    df['ROC_10'] = df['Close'].pct_change(periods=10)\n",
    "    \n",
    "    # Volatility Indicators with min_periods\n",
    "    df['Volatility_5'] = df['Close'].rolling(window=5, min_periods=1).std()\n",
    "    df['Volatility_10'] = df['Close'].rolling(window=10, min_periods=1).std()\n",
    "    \n",
    "    # Relative Strength Index (RSI) with min_periods\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    average_gain = gain.rolling(window=14, min_periods=1).mean()\n",
    "    average_loss = loss.rolling(window=14, min_periods=1).mean()\n",
    "    rs = average_gain / (average_loss + 1e-10)  # Add small constant to avoid division by zero\n",
    "    df['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False, min_periods=1).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False, min_periods=1).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False, min_periods=1).mean()\n",
    "    df['MACD_diff'] = df['MACD'] - df['MACD_signal']\n",
    "    \n",
    "    # Bollinger Bands with min_periods\n",
    "    df['Middle_Band'] = df['Close'].rolling(window=20, min_periods=1).mean()\n",
    "    df['Std_Dev'] = df['Close'].rolling(window=20, min_periods=1).std()\n",
    "    df['Upper_Band'] = df['Middle_Band'] + (df['Std_Dev'] * 2)\n",
    "    df['Lower_Band'] = df['Middle_Band'] - (df['Std_Dev'] * 2)\n",
    "    df['Bollinger_Width'] = df['Upper_Band'] - df['Lower_Band']\n",
    "    \n",
    "    # Percent B (%B) Indicator\n",
    "    df['Percent_B'] = (df['Close'] - df['Lower_Band']) / (df['Upper_Band'] - df['Lower_Band'] + 1e-10)\n",
    "    \n",
    "    # Simplified Williams %R with min_periods\n",
    "    df['Highest_Close_14'] = df['Close'].rolling(window=14, min_periods=1).max()\n",
    "    df['Lowest_Close_14'] = df['Close'].rolling(window=14, min_periods=1).min()\n",
    "    df['Williams_%R'] = ((df['Highest_Close_14'] - df['Close']) / (df['Highest_Close_14'] - df['Lowest_Close_14'] + 1e-10)) * -100\n",
    "    \n",
    "    # Exponential Moving Average Differences\n",
    "    df['EMA_5_10_Diff'] = df['EMA_5'] - df['EMA_10']\n",
    "    df['EMA_5_20_Diff'] = df['EMA_5'] - df['EMA_20']\n",
    "    \n",
    "    # Lag Features\n",
    "    df['Lag_Close_1'] = df['Close'].shift(1)\n",
    "    df['Lag_Close_2'] = df['Close'].shift(2)\n",
    "    df['Lag_Close_3'] = df['Close'].shift(3)\n",
    "    \n",
    "    # Rolling Statistics with min_periods\n",
    "    df['Rolling_Skew_Close_5'] = df['Close'].rolling(window=5, min_periods=1).skew()\n",
    "    df['Rolling_Kurt_Close_5'] = df['Close'].rolling(window=5, min_periods=1).kurt()\n",
    "    \n",
    "    # Handle NaN values appropriately\n",
    "    # Replace deprecated fillna methods with ffill() and bfill()\n",
    "    df.ffill(inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    \n",
    "    # Reset index after processing\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to each stock's daily data\n",
    "for stock in stocks:\n",
    "    df_daily = daily_data_dict[stock].copy()\n",
    "    \n",
    "    # Reset index if 'Date' is not a column\n",
    "    if 'Date' not in df_daily.columns:\n",
    "        df_daily.reset_index(inplace=True)\n",
    "    \n",
    "    # Ensure 'Date' is of datetime type\n",
    "    df_daily['Date'] = pd.to_datetime(df_daily['Date'])\n",
    "    \n",
    "    # Sort by 'Date'\n",
    "    df_daily.sort_values('Date', inplace=True)\n",
    "    df_daily.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Add enhanced 'Close' price-based features\n",
    "    df_daily_with_features = add_close_price_features(df_daily)\n",
    "    \n",
    "    # Log information\n",
    "    logging.info(f\"'Close' price-based technical indicators added for daily data of {stock}\")\n",
    "    logging.info(f\"Sample features for {stock}:\\n{df_daily_with_features.tail(5)}\")\n",
    "    \n",
    "    # Update the dictionary\n",
    "    daily_data_dict[stock] = df_daily_with_features\n",
    "\n",
    "print(\"Feature engineering complete. Data is ready for splitting into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511950f3-3177-4ab3-a9fb-299b9ab837e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Splitting Data into Training and Testing Sets (Modified to Include 'test_dates')\n",
    "\n",
    "# Function to split time series data\n",
    "def split_time_series_data(df, date_column='Date', target_column='Close', split_ratio=0.8):\n",
    "    # Sort the DataFrame by the date/time column\n",
    "    df_sorted = df.sort_values(by=date_column).reset_index(drop=True)\n",
    "    \n",
    "    # Determine the split index\n",
    "    split_index = int(len(df_sorted) * split_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_df = df_sorted.iloc[:split_index]\n",
    "    test_df = df_sorted.iloc[split_index:]\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = [col for col in df.columns if col not in [date_column, target_column]]\n",
    "    \n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_column]\n",
    "    \n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[target_column]\n",
    "    \n",
    "    # Extract 'test_dates'\n",
    "    test_dates = test_df[date_column].reset_index(drop=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols, test_dates\n",
    "\n",
    "# Initialize a dictionary to hold split data for each stock\n",
    "split_data_dict = {}\n",
    "\n",
    "# Apply the function to each stock's data\n",
    "for stock in stocks:\n",
    "    df = daily_data_dict[stock].copy()\n",
    "    \n",
    "    # Verify that the DataFrame is not empty\n",
    "    if df.empty:\n",
    "        logging.warning(f\"The DataFrame for {stock} is empty. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test, feature_cols, test_dates = split_time_series_data(\n",
    "        df,\n",
    "        date_column='Date',\n",
    "        target_column='Close',\n",
    "        split_ratio=0.8\n",
    "    )\n",
    "    \n",
    "    # Store the split data in the dictionary\n",
    "    split_data_dict[stock] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_columns': feature_cols,\n",
    "        'test_dates': test_dates  # Include 'test_dates'\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Data for {stock} has been split into training and testing sets.\")\n",
    "\n",
    "# Function to verify the split data\n",
    "def verify_split(split_dict, stocks):\n",
    "    for stock in stocks:\n",
    "        data = split_dict.get(stock)\n",
    "        if data is None:\n",
    "            print(f\"No data found for {stock}.\")\n",
    "            continue\n",
    "        \n",
    "        X_train = data.get('X_train')\n",
    "        X_test = data.get('X_test')\n",
    "        y_train = data.get('y_train')\n",
    "        y_test = data.get('y_test')\n",
    "        feature_columns = data.get('feature_columns', [])\n",
    "        test_dates = data.get('test_dates', None)\n",
    "        \n",
    "        # Check if any of the datasets are None or empty\n",
    "        if X_train is None or X_train.empty:\n",
    "            print(f\"X_train is None or empty for {stock}.\")\n",
    "            continue\n",
    "        if X_test is None or X_test.empty:\n",
    "            print(f\"X_test is None or empty for {stock}.\")\n",
    "            continue\n",
    "        if y_train is None or y_train.empty:\n",
    "            print(f\"y_train is None or empty for {stock}.\")\n",
    "            continue\n",
    "        if y_test is None or y_test.empty:\n",
    "            print(f\"y_test is None or empty for {stock}.\")\n",
    "            continue\n",
    "        if test_dates is None or test_dates.empty:\n",
    "            print(f\"test_dates is None or empty for {stock}.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Verifying data split for {stock}:\")\n",
    "        print(f\" - Training set size: {X_train.shape[0]} samples\")\n",
    "        print(f\" - Testing set size: {X_test.shape[0]} samples\")\n",
    "        print(f\" - Number of features: {len(feature_columns)}\")\n",
    "        print(f\" - Feature columns:\\n{feature_columns}\")\n",
    "        print(f\" - Test Dates (first 5): {test_dates.head().tolist()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Verify the data splitting for each stock\n",
    "print(\"\\nVerifying Data Splitting:\")\n",
    "verify_split(split_data_dict, stocks)\n",
    "\n",
    "print(\"\\nData splitting complete. Ready for scaling in the next cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f740aa-6edf-4a11-a4be-e704ad21db5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Scaling the Data Using MinMaxScaler\n",
    "\n",
    "# Function to scale data using MinMaxScaler for both features and targets\n",
    "def scale_data_with_target(split_data_dict):\n",
    "    scaled_data_dict = {}\n",
    "    \n",
    "    # Directories to save scalers\n",
    "    scaler_save_dir = '../models/scalers'\n",
    "    os.makedirs(scaler_save_dir, exist_ok=True)\n",
    "    \n",
    "    for stock, data in split_data_dict.items():\n",
    "        logging.info(f\"Scaling data for {stock}...\")\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        X_test = data['X_test']\n",
    "        y_train = data['y_train'].values.reshape(-1, 1)  # Reshape for scaler\n",
    "        y_test = data['y_test'].values.reshape(-1, 1)\n",
    "        test_dates = data.get('test_dates')  # Retrieve 'test_dates'\n",
    "        \n",
    "        # Initialize scalers\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        \n",
    "        # Fit scalers on training data and transform both training and testing data\n",
    "        X_train_scaled = pd.DataFrame(scaler_X.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(scaler_X.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "        \n",
    "        y_train_scaled = scaler_y.fit_transform(y_train).flatten()  # Flatten to 1D array\n",
    "        y_test_scaled = scaler_y.transform(y_test).flatten()\n",
    "        \n",
    "        # Save scalers using joblib\n",
    "        scaler_X_path = os.path.join(scaler_save_dir, f'minmax_scaler_X_{stock}.joblib')\n",
    "        scaler_y_path = os.path.join(scaler_save_dir, f'minmax_scaler_y_{stock}.joblib')\n",
    "        joblib.dump(scaler_X, scaler_X_path)\n",
    "        joblib.dump(scaler_y, scaler_y_path)\n",
    "        logging.info(f\"Scalers saved for {stock} at {scaler_X_path} and {scaler_y_path}.\")\n",
    "        \n",
    "        # ----- Include 'test_dates' -----\n",
    "        if test_dates is not None and not test_dates.empty:\n",
    "            logging.info(f\"'test_dates' found for {stock}. Including in scaled data.\")\n",
    "        else:\n",
    "            logging.warning(f\"No 'test_dates' found for {stock}. Creating dummy dates.\")\n",
    "            test_dates = pd.date_range(start='2020-01-01', periods=len(y_test_scaled), freq='D')\n",
    "        \n",
    "        # Update the scaled data dictionary with 'test_dates'\n",
    "        scaled_data_dict[stock] = {\n",
    "            'X_train_scaled': X_train_scaled,\n",
    "            'X_test_scaled': X_test_scaled,\n",
    "            'y_train_scaled': y_train_scaled,\n",
    "            'y_test_scaled': y_test_scaled,\n",
    "            'scaler_X': scaler_X,\n",
    "            'scaler_y': scaler_y,\n",
    "            'feature_columns': data['feature_columns'],\n",
    "            'test_dates': test_dates  # Include 'test_dates'\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Completed scaling for {stock}.\")\n",
    "    \n",
    "    return scaled_data_dict\n",
    "\n",
    "# Scale the split daily data with target\n",
    "logging.info(\"Starting to scale Daily Data with target...\")\n",
    "scaled_daily_data = scale_data_with_target(split_data_dict)\n",
    "logging.info(\"Completed scaling Daily Data with target.\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nVerifying Scaled Data for Daily Data (Including 'test_dates'):\")\n",
    "for stock in scaled_daily_data.keys():\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    test_dates = data['test_dates']\n",
    "    \n",
    "    print(f\"Scaled data for {stock}:\")\n",
    "    print(f\" - Scaled Training set shape: {X_train_scaled.shape}, Scaled Training targets shape: {y_train_scaled.shape}\")\n",
    "    print(f\" - Scaled Testing set shape: {X_test_scaled.shape}, Scaled Testing targets shape: {y_test_scaled.shape}\")\n",
    "    print(f\" - Feature columns: {X_train_scaled.columns.tolist()}\")\n",
    "    \n",
    "    # Handle 'test_dates' based on its type\n",
    "    if isinstance(test_dates, pd.DatetimeIndex):\n",
    "        # Slice the first five dates and convert to a list\n",
    "        test_dates_list = test_dates[:5].tolist()\n",
    "    elif isinstance(test_dates, pd.Series):\n",
    "        # Use head() if it's a Series\n",
    "        test_dates_list = test_dates.head(5).tolist()\n",
    "    else:\n",
    "        # Convert to list and slice if it's another type\n",
    "        test_dates_list = list(test_dates)[:5]\n",
    "    \n",
    "    print(f\" - Test Dates (first 5): {test_dates_list}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b973a-a31f-49e0-a6c0-e30aa9181a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Training and Evaluating LSTM Models for Daily Data\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Define Parameters\n",
    "TIMESTEPS = 60  # Number of past days to use for prediction\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # Increased to allow more training\n",
    "VALIDATION_SPLIT = 0.1  # Fraction of training data to use for validation\n",
    "\n",
    "# Define Evaluation Metrics Function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Function to Create Sequences\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to Build LSTM Model\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape)) # Explicit Input Layer\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Initialize a dictionary to store model performance\n",
    "model_performance = {}\n",
    "\n",
    "# Directories to save models and scalers\n",
    "model_save_dir = '../models/lstm_models'\n",
    "scaler_save_dir = '../models/scalers'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "os.makedirs(scaler_save_dir, exist_ok=True)\n",
    "\n",
    "# Iterate Through Each Stock\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining LSTM Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_X = data['scaler_X']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Create Sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    print(f\" - Training sequences: {X_train_seq.shape}, Training targets: {y_train_seq.shape}\")\n",
    "    print(f\" - Testing sequences: {X_test_seq.shape}, Testing targets: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Build the Model\n",
    "    model = build_lstm_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
    "    model.summary()\n",
    "    \n",
    "    # Define Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(model_save_dir, f'lstm_{stock}_best.keras'),  # Changed to .keras\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the Model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the Best Model\n",
    "    best_model_path = os.path.join(model_save_dir, f'lstm_{stock}_best.keras')\n",
    "    model = load_model(best_model_path)\n",
    "    print(f\" - Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = model.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Ensure Consistent Lengths\n",
    "    print(f\" - Length of y_test: {len(y_test)}\")\n",
    "    print(f\" - Length of predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    rmse, mae, r2 = evaluate_model(y_test, predictions)\n",
    "    model_performance[stock] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "    \n",
    "    print(f\" - Evaluation Metrics for {stock}: RMSE = {rmse:.4f}, MAE = {mae:.4f}, R2 = {r2:.4f}\")\n",
    "    \n",
    "    print(f\"Model training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Summary of Model Performance\n",
    "print(f\"\\n{'='*50}\\nSummary of Model Performance\\n{'='*50}\")\n",
    "for stock, metrics in model_performance.items():\n",
    "    print(f\"{stock}: RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}, R2 = {metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720ab5c-ae48-4e11-842b-2bcf7da5c7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Training and Evaluating GRU Models for Daily Data\n",
    "\n",
    "# Define Parameters\n",
    "TIMESTEPS = 60  # Number of past days to use for prediction\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Function to Create Sequences (Already Defined in Cell 6)\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to Build GRU Model\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(GRU(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Initialize a dictionary to store GRU model performance\n",
    "gru_model_performance = {}\n",
    "\n",
    "# Define Directory to Save GRU Models\n",
    "gru_model_save_dir = '../models/gru_models'\n",
    "os.makedirs(gru_model_save_dir, exist_ok=True)\n",
    "\n",
    "# Iterate Through Each Stock\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining GRU Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Create Sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    print(f\" - Training sequences: {X_train_seq.shape}, Training targets: {y_train_seq.shape}\")\n",
    "    print(f\" - Testing sequences: {X_test_seq.shape}, Testing targets: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Build the GRU Model\n",
    "    model = build_gru_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
    "    model.summary()\n",
    "    \n",
    "    # Define Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(gru_model_save_dir, f'gru_{stock}_best.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the GRU Model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the Best Model\n",
    "    best_model_path = os.path.join(gru_model_save_dir, f'gru_{stock}_best.keras')\n",
    "    if os.path.exists(best_model_path):\n",
    "        model = load_model(best_model_path)\n",
    "        print(f\" - Loaded best model from {best_model_path}\")\n",
    "    else:\n",
    "        print(f\" - Best GRU model for {stock} not found at {best_model_path}.\")\n",
    "        continue  # Skip evaluation if model not saved\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = model.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Ensure Consistent Lengths\n",
    "    print(f\" - Length of y_test: {len(y_test)}\")\n",
    "    print(f\" - Length of predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    gru_model_performance[stock] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "    \n",
    "    print(f\" - Evaluation Metrics for GRU {stock}: RMSE = {rmse:.4f}, MAE = {mae:.4f}, R2 = {r2:.4f}\")\n",
    "    \n",
    "    print(f\"GRU model training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Summary of GRU Model Performance\n",
    "print(f\"\\n{'='*50}\\nSummary of GRU Model Performance\\n{'='*50}\")\n",
    "for stock, metrics in gru_model_performance.items():\n",
    "    print(f\"{stock}: RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}, R2 = {metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bb105-329c-443d-a3a0-bb1bd94f072d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Training and Evaluating XGBoost Models\n",
    "\n",
    "# Define Parameters\n",
    "TIMESTEPS = 60  # Ensure consistency with LSTM\n",
    "model_save_dir = '../models/xgb_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "overall_metrics_xgb = {}\n",
    "grouped_metrics_all_xgb = {'Month': {}, 'Quarter': {}, 'Season': {}}\n",
    "\n",
    "# Function to Add Time Features\n",
    "def add_time_features_xgb(eval_df):\n",
    "    eval_df['Month'] = eval_df['Date'].dt.month\n",
    "    eval_df['Quarter'] = eval_df['Date'].dt.quarter\n",
    "    eval_df['Season'] = eval_df['Month'].apply(\n",
    "        lambda month: 'Winter' if month in [12, 1, 2] else\n",
    "                      'Spring' if month in [3, 4, 5] else\n",
    "                      'Summer' if month in [6, 7, 8] else\n",
    "                      'Autumn'\n",
    "    )\n",
    "    return eval_df\n",
    "\n",
    "# Iterate Through Each Stock for Evaluation and Plotting\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining and Evaluating XGBoost Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "\n",
    "    # Initialize and Train XGBoost Regressor\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train_scaled, y_train_scaled)\n",
    "    print(f\" - XGBoost model trained for {stock}\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create Evaluation DataFrame\n",
    "    if isinstance(X_test_scaled, pd.DataFrame) and isinstance(X_test_scaled.index, pd.DatetimeIndex):\n",
    "        dates = X_test_scaled.index\n",
    "    elif isinstance(X_test_scaled, pd.DataFrame) and 'Date' in X_test_scaled.columns:\n",
    "        dates = pd.to_datetime(X_test_scaled['Date'])\n",
    "    else:\n",
    "        print(f\" - No Date information found for {stock}. Creating dummy dates.\")\n",
    "        dates = pd.date_range(start='2020-01-01', periods=len(y_test), freq='D')\n",
    "    \n",
    "    eval_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    \n",
    "    # Add Time Features\n",
    "    eval_df = add_time_features_xgb(eval_df)\n",
    "    \n",
    "    # Calculate Overall Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df['Actual'], eval_df['Predicted']))\n",
    "    mae = mean_absolute_error(eval_df['Actual'], eval_df['Predicted'])\n",
    "    r2 = r2_score(eval_df['Actual'], eval_df['Predicted'])\n",
    "    mape = mean_absolute_percentage_error(eval_df['Actual'], eval_df['Predicted']) * 100\n",
    "    \n",
    "    overall_metrics_xgb[stock] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(f\" - Overall Evaluation Metrics for {stock}:\")\n",
    "    print(f\"    RMSE = {rmse:.4f}\")\n",
    "    print(f\"    MAE = {mae:.4f}\")\n",
    "    print(f\"    R2 = {r2:.4f}\")\n",
    "    print(f\"    MAPE = {mape:.2f}%\")\n",
    "    \n",
    "    # Calculate Grouped Metrics\n",
    "    grouped_metrics_month = eval_df.groupby('Month').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_quarter = eval_df.groupby('Quarter').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_season = eval_df.groupby('Season').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_all_xgb['Month'][stock] = grouped_metrics_month\n",
    "    grouped_metrics_all_xgb['Quarter'][stock] = grouped_metrics_quarter\n",
    "    grouped_metrics_all_xgb['Season'][stock] = grouped_metrics_season\n",
    "\n",
    "    # Save the trained model\n",
    "    model_save_path = os.path.join(model_save_dir, f'xgb_{stock}_model.json')\n",
    "    xgb_model.save_model(model_save_path)\n",
    "    print(f\" - XGBoost model saved for {stock} at {model_save_path}\")\n",
    "    \n",
    "    print(f\"Training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Create Overall Metrics Table\n",
    "overall_metrics_xgb_df = pd.DataFrame(overall_metrics_xgb).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks - XGBoost\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_xgb_df)\n",
    "overall_metrics_xgb_df.to_csv('overall_evaluation_metrics_xgb.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table for XGBoost saved as 'overall_evaluation_metrics_xgb.csv'.\")\n",
    "\n",
    "# Function to Create Grouped Metrics Tables\n",
    "def create_grouped_metrics_tables_xgb(grouped_metrics_all_xgb, grouping):\n",
    "    grouped_metrics_tables_xgb = {}\n",
    "    for stock, metrics in grouped_metrics_all_xgb[grouping].items():\n",
    "        metrics_df = metrics.reset_index()\n",
    "        metrics_df.columns = [grouping] + list(metrics_df.columns[1:])\n",
    "        grouped_metrics_tables_xgb[stock] = metrics_df\n",
    "    return grouped_metrics_tables_xgb\n",
    "\n",
    "# Create and Save Grouped Metrics Tables\n",
    "for grouping in ['Month', 'Quarter', 'Season']:\n",
    "    grouped_tables_xgb = create_grouped_metrics_tables_xgb(grouped_metrics_all_xgb, grouping)\n",
    "    for stock, table in grouped_tables_xgb.items():\n",
    "        print(f\"\\n{'='*50}\\n{grouping} Evaluation Metrics for {stock} - XGBoost\\n{'='*50}\")\n",
    "        display(table)\n",
    "        filename = f'{stock}_{grouping}_evaluation_metrics_xgb.csv'\n",
    "        table.to_csv(filename, index=False)\n",
    "        print(f\" - {grouping} Evaluation Metrics table for {stock} saved as '{filename}'.\")\n",
    "    \n",
    "    # Create Comparative Metrics Tables Across Stocks\n",
    "    for metric in ['RMSE', 'MAE', 'R2', 'MAPE']:\n",
    "        comparative_df_xgb = pd.DataFrame({stock: grouped_metrics_all_xgb[grouping][stock][metric] for stock in grouped_metrics_all_xgb[grouping].keys()})\n",
    "        comparative_df_xgb.index.name = grouping\n",
    "        print(f\"\\n{'='*50}\\nComparative {metric} Across {grouping} for All Stocks - XGBoost\\n{'='*50}\")\n",
    "        display(comparative_df_xgb)\n",
    "        filename = f'comparative_{metric}_across_{grouping}_xgb.csv'\n",
    "        comparative_df_xgb.to_csv(filename)\n",
    "        print(f\" - Comparative {metric} Across {grouping} table for XGBoost saved as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa9342-cdf6-4acb-8bcb-d0ce6bcdbd43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Training and Evaluating Random Forest Models\n",
    "\n",
    "# Define Parameters\n",
    "model_save_dir = '../models/random_forest_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "overall_metrics_rf = {}\n",
    "grouped_metrics_all_rf = {'Month': {}, 'Quarter': {}, 'Season': {}}\n",
    "\n",
    "# Function to Add Time Features (if not already added)\n",
    "def add_time_features_rf(eval_df):\n",
    "    eval_df['Month'] = eval_df['Date'].dt.month\n",
    "    eval_df['Quarter'] = eval_df['Date'].dt.quarter\n",
    "    eval_df['Season'] = eval_df['Month'].apply(\n",
    "        lambda month: 'Winter' if month in [12, 1, 2] else\n",
    "                      'Spring' if month in [3, 4, 5] else\n",
    "                      'Summer' if month in [6, 7, 8] else\n",
    "                      'Autumn'\n",
    "    )\n",
    "    return eval_df\n",
    "\n",
    "# Iterate Through Each Stock for Evaluation and Plotting\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining and Evaluating Random Forest Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Initialize and Train Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train_scaled)\n",
    "    print(f\" - Random Forest model trained for {stock}\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Retrieve 'test_dates' from scaled_daily_data\n",
    "    test_dates = data.get('test_dates')\n",
    "    \n",
    "    # Create Evaluation DataFrame using 'test_dates'\n",
    "    if test_dates is not None and len(test_dates) == len(y_test):\n",
    "        dates = test_dates\n",
    "    else:\n",
    "        print(f\" - No 'test_dates' found for {stock}. Creating dummy dates.\")\n",
    "        dates = pd.date_range(start='2020-01-01', periods=len(y_test), freq='D')\n",
    "    \n",
    "    eval_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    \n",
    "    # Add Time Features\n",
    "    eval_df = add_time_features_rf(eval_df)\n",
    "    \n",
    "    # Calculate Overall Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df['Actual'], eval_df['Predicted']))\n",
    "    mae = mean_absolute_error(eval_df['Actual'], eval_df['Predicted'])\n",
    "    r2 = r2_score(eval_df['Actual'], eval_df['Predicted'])\n",
    "    mape = mean_absolute_percentage_error(eval_df['Actual'], eval_df['Predicted']) * 100\n",
    "    \n",
    "    overall_metrics_rf[stock] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(f\" - Overall Evaluation Metrics for {stock}:\")\n",
    "    print(f\"    RMSE = {rmse:.4f}\")\n",
    "    print(f\"    MAE = {mae:.4f}\")\n",
    "    print(f\"    R2 = {r2:.4f}\")\n",
    "    print(f\"    MAPE = {mape:.2f}%\")\n",
    "    \n",
    "    # Calculate Grouped Metrics\n",
    "    grouped_metrics_month = eval_df.groupby('Month').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_quarter = eval_df.groupby('Quarter').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_season = eval_df.groupby('Season').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_all_rf['Month'][stock] = grouped_metrics_month\n",
    "    grouped_metrics_all_rf['Quarter'][stock] = grouped_metrics_quarter\n",
    "    grouped_metrics_all_rf['Season'][stock] = grouped_metrics_season\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_save_path = os.path.join(model_save_dir, f'rf_{stock}_model.pkl')\n",
    "    joblib.dump(rf_model, model_save_path)\n",
    "    print(f\" - Random Forest model saved for {stock} at {model_save_path}\")\n",
    "    \n",
    "    print(f\"Training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Create Overall Metrics Table\n",
    "overall_metrics_rf_df = pd.DataFrame(overall_metrics_rf).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks - Random Forest\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_rf_df)\n",
    "overall_metrics_rf_df.to_csv('overall_evaluation_metrics_rf.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table for Random Forest saved as 'overall_evaluation_metrics_rf.csv'.\")\n",
    "\n",
    "# Function to Create Grouped Metrics Tables\n",
    "def create_grouped_metrics_tables_rf(grouped_metrics_all_rf, grouping):\n",
    "    grouped_metrics_tables_rf = {}\n",
    "    for stock, metrics in grouped_metrics_all_rf[grouping].items():\n",
    "        metrics_df = metrics.reset_index()\n",
    "        metrics_df.columns = [grouping] + list(metrics_df.columns[1:])\n",
    "        grouped_metrics_tables_rf[stock] = metrics_df\n",
    "    return grouped_metrics_tables_rf\n",
    "\n",
    "# Create and Save Grouped Metrics Tables\n",
    "for grouping in ['Month', 'Quarter', 'Season']:\n",
    "    grouped_tables_rf = create_grouped_metrics_tables_rf(grouped_metrics_all_rf, grouping)\n",
    "    for stock, table in grouped_tables_rf.items():\n",
    "        print(f\"\\n{'='*50}\\n{grouping} Evaluation Metrics for {stock} - Random Forest\\n{'='*50}\")\n",
    "        display(table)\n",
    "        filename = f'{stock}_{grouping}_evaluation_metrics_rf.csv'\n",
    "        table.to_csv(filename, index=False)\n",
    "        print(f\" - {grouping} Evaluation Metrics table for {stock} saved as '{filename}'.\")\n",
    "    \n",
    "    # Create Comparative Metrics Tables Across Stocks\n",
    "    for metric in ['RMSE', 'MAE', 'R2', 'MAPE']:\n",
    "        comparative_df_rf = pd.DataFrame({stock: grouped_metrics_all_rf[grouping][stock][metric] for stock in grouped_metrics_all_rf[grouping].keys()})\n",
    "        comparative_df_rf.index.name = grouping\n",
    "        print(f\"\\n{'='*50}\\nComparative {metric} Across {grouping} for All Stocks - Random Forest\\n{'='*50}\")\n",
    "        display(comparative_df_rf)\n",
    "        filename = f'comparative_{metric}_across_{grouping}_rf.csv'\n",
    "        comparative_df_rf.to_csv(filename)\n",
    "        print(f\" - Comparative {metric} Across {grouping} table for Random Forest saved as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddb11f-7a92-4407-ad3f-0d806d529537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11A: Generating Meta-Features from Base Models\n",
    "\n",
    "# Define Paths for Models\n",
    "xgb_model_dir = '../models/xgb_models'\n",
    "rf_model_dir = '../models/random_forest_models'\n",
    "lstm_model_dir = '../models/lstm_models'\n",
    "gru_model_dir = '../models/gru_models'\n",
    "meta_model_dir = '../models/meta_model'\n",
    "os.makedirs(meta_model_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Dictionaries to Store Meta-Features and Targets\n",
    "meta_features_train_dict = {}\n",
    "meta_features_test_dict = {}\n",
    "y_train_dict = {}\n",
    "y_test_dict = {}\n",
    "\n",
    "# Function to Create Sequences (Assuming it's defined in a common cell)\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Iterate Through Each Stock to Populate Meta-Features\n",
    "for stock in stocks:\n",
    "    print(f\"\\n{'='*50}\\nProcessing Stock: {stock}\\n{'='*50}\")\n",
    "\n",
    "    # ----- Load Base Models -----\n",
    "    missing_models = []\n",
    "    \n",
    "    # Load XGBoost Model\n",
    "    xgb_model_path = os.path.join(xgb_model_dir, f'xgb_{stock}_model.json')\n",
    "    if not os.path.exists(xgb_model_path):\n",
    "        missing_models.append('XGBoost')\n",
    "    else:\n",
    "        try:\n",
    "            xgb_model = xgb.XGBRegressor()\n",
    "            xgb_model.load_model(xgb_model_path)\n",
    "            print(f\" - Loaded XGBoost model for {stock}.\")\n",
    "        except Exception as e:\n",
    "            print(f\" - Error loading XGBoost model for {stock}: {e}\")\n",
    "            missing_models.append('XGBoost')\n",
    "    \n",
    "    # Load Random Forest Model\n",
    "    rf_model_path = os.path.join(rf_model_dir, f'rf_{stock}_model.pkl')\n",
    "    if not os.path.exists(rf_model_path):\n",
    "        missing_models.append('Random Forest')\n",
    "    else:\n",
    "        try:\n",
    "            rf_model = joblib.load(rf_model_path)\n",
    "            print(f\" - Loaded Random Forest model for {stock}.\")\n",
    "        except Exception as e:\n",
    "            print(f\" - Error loading Random Forest model for {stock}: {e}\")\n",
    "            missing_models.append('Random Forest')\n",
    "\n",
    "    # Load LSTM Model\n",
    "    lstm_model_path = os.path.join(lstm_model_dir, f'lstm_{stock}_best.keras')\n",
    "    if not os.path.exists(lstm_model_path):\n",
    "        missing_models.append('LSTM')\n",
    "    else:\n",
    "        try:\n",
    "            lstm_model = load_model(lstm_model_path)\n",
    "            print(f\" - Loaded LSTM model for {stock}.\")\n",
    "        except Exception as e:\n",
    "            print(f\" - Error loading LSTM model for {stock}: {e}\")\n",
    "            missing_models.append('LSTM')\n",
    "\n",
    "    # Load GRU Model\n",
    "    gru_model_path = os.path.join(gru_model_dir, f'gru_{stock}_best.keras')\n",
    "    if not os.path.exists(gru_model_path):\n",
    "        missing_models.append('GRU')\n",
    "    else:\n",
    "        try:\n",
    "            gru_model = load_model(gru_model_path)\n",
    "            print(f\" - Loaded GRU model for {stock}.\")\n",
    "        except Exception as e:\n",
    "            print(f\" - Error loading GRU model for {stock}: {e}\")\n",
    "            missing_models.append('GRU')\n",
    "\n",
    "    if missing_models:\n",
    "        print(f\" - Missing or failed to load models for {stock}: {', '.join(missing_models)}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ----- Retrieve Scaled Data -----\n",
    "    data = scaled_daily_data.get(stock)\n",
    "    if data is None:\n",
    "        print(f\" - No scaled data found for {stock}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Check for 'test_dates'\n",
    "    if 'test_dates' not in data:\n",
    "        print(f\" - 'test_dates' not found for {stock}. Skipping.\")\n",
    "        continue\n",
    "    test_dates = data['test_dates']\n",
    "\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "\n",
    "    # ----- Create Sequences for LSTM and GRU Models -----\n",
    "    TIMESTEPS = 60  # Ensure consistency\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    print(f\" - Training sequences: {X_train_seq.shape}, Training targets: {y_train_seq.shape}\")\n",
    "    print(f\" - Testing sequences: {X_test_seq.shape}, Testing targets: {y_test_seq.shape}\")\n",
    "    \n",
    "    # ----- Generate Predictions from Base Models -----\n",
    "    # Initialize lists to store predictions\n",
    "    try:\n",
    "        # XGBoost Predictions\n",
    "        xgb_pred_train_scaled = xgb_model.predict(X_train_scaled.iloc[TIMESTEPS:])\n",
    "        xgb_pred_test_scaled = xgb_model.predict(X_test_scaled.iloc[TIMESTEPS:])\n",
    "        xgb_pred_train = scaler_y.inverse_transform(xgb_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "        xgb_pred_test = scaler_y.inverse_transform(xgb_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "        print(f\" - XGBoost predictions generated for {stock}.\")\n",
    "        \n",
    "        # Random Forest Predictions\n",
    "        rf_pred_train_scaled = rf_model.predict(X_train_scaled.iloc[TIMESTEPS:])\n",
    "        rf_pred_test_scaled = rf_model.predict(X_test_scaled.iloc[TIMESTEPS:])\n",
    "        rf_pred_train = scaler_y.inverse_transform(rf_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "        rf_pred_test = scaler_y.inverse_transform(rf_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "        print(f\" - Random Forest predictions generated for {stock}.\")\n",
    "        \n",
    "        # LSTM Predictions\n",
    "        lstm_pred_train_scaled = lstm_model.predict(X_train_seq).flatten()\n",
    "        lstm_pred_test_scaled = lstm_model.predict(X_test_seq).flatten()\n",
    "        lstm_pred_train = scaler_y.inverse_transform(lstm_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "        lstm_pred_test = scaler_y.inverse_transform(lstm_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "        print(f\" - LSTM predictions generated for {stock}.\")\n",
    "        \n",
    "        # GRU Predictions\n",
    "        gru_pred_train_scaled = gru_model.predict(X_train_seq).flatten()\n",
    "        gru_pred_test_scaled = gru_model.predict(X_test_seq).flatten()\n",
    "        gru_pred_train = scaler_y.inverse_transform(gru_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "        gru_pred_test = scaler_y.inverse_transform(gru_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "        print(f\" - GRU predictions generated for {stock}.\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error during prediction generation for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # ----- Align Predictions and Targets -----\n",
    "    # Determine the minimum length to ensure alignment\n",
    "    min_length_train = min(len(xgb_pred_train), len(rf_pred_train), len(lstm_pred_train), len(gru_pred_train), len(y_train_seq))\n",
    "    min_length_test = min(len(xgb_pred_test), len(rf_pred_test), len(lstm_pred_test), len(gru_pred_test), len(y_test_seq))\n",
    "    \n",
    "    # Slice predictions and targets to min_length\n",
    "    xgb_pred_train = xgb_pred_train[:min_length_train]\n",
    "    rf_pred_train = rf_pred_train[:min_length_train]\n",
    "    lstm_pred_train = lstm_pred_train[:min_length_train]\n",
    "    gru_pred_train = gru_pred_train[:min_length_train]\n",
    "    y_train = scaler_y.inverse_transform(y_train_seq[:min_length_train].reshape(-1, 1)).flatten()\n",
    "    \n",
    "    xgb_pred_test = xgb_pred_test[:min_length_test]\n",
    "    rf_pred_test = rf_pred_test[:min_length_test]\n",
    "    lstm_pred_test = lstm_pred_test[:min_length_test]\n",
    "    gru_pred_test = gru_pred_test[:min_length_test]\n",
    "    y_test = scaler_y.inverse_transform(y_test_seq[:min_length_test].reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # ----- Populate Meta-Features for Training Data -----\n",
    "    meta_features_train = pd.DataFrame({\n",
    "        'XGB_Pred': xgb_pred_train,\n",
    "        'RF_Pred': rf_pred_train,\n",
    "        'LSTM_Pred': lstm_pred_train,\n",
    "        'GRU_Pred': gru_pred_train\n",
    "    })\n",
    "    meta_features_train_dict[stock] = meta_features_train\n",
    "    print(f\" - Meta-features for training data populated for {stock}.\")\n",
    "    \n",
    "    # ----- Populate Meta-Features for Test Data -----\n",
    "    meta_features_test = pd.DataFrame({\n",
    "        'XGB_Pred': xgb_pred_test,\n",
    "        'RF_Pred': rf_pred_test,\n",
    "        'LSTM_Pred': lstm_pred_test,\n",
    "        'GRU_Pred': gru_pred_test\n",
    "    })\n",
    "    meta_features_test_dict[stock] = meta_features_test\n",
    "    print(f\" - Meta-features for testing data populated for {stock}.\")\n",
    "    \n",
    "    # ----- Store Target Variables -----\n",
    "    y_train_dict[stock] = y_train\n",
    "    y_test_dict[stock] = y_test\n",
    "    print(f\" - Target variables stored for {stock}.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nMeta-Features DataFrame Shapes\\n\" + \"=\"*50)\n",
    "for stock in meta_features_train_dict.keys():\n",
    "    print(f\" - {stock}: meta_features_train shape: {meta_features_train_dict[stock].shape}, meta_features_test shape: {meta_features_test_dict[stock].shape}, y_train shape: {y_train_dict[stock].shape}, y_test shape: {y_test_dict[stock].shape}\")\n",
    "print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7b9b8-c188-4262-8c64-6fdd430e259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11B: Training and Evaluating the Stacking Meta-Model\n",
    "\n",
    "# Initialize a dictionary to store meta-model performance\n",
    "meta_model_per_stock = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTraining Meta-Models for Each Stock\\n\" + \"=\"*50)\n",
    "\n",
    "for stock in meta_features_train_dict.keys():\n",
    "    print(f\"\\nTraining Meta-Model for {stock}\")\n",
    "    \n",
    "    meta_features_train = meta_features_train_dict[stock]\n",
    "    meta_features_test = meta_features_test_dict[stock]\n",
    "    y_train = y_train_dict[stock]\n",
    "    y_test = y_test_dict[stock]\n",
    "    \n",
    "    # Check if meta_features_train and y_train are non-empty\n",
    "    if meta_features_train.empty or len(y_train) == 0:\n",
    "        print(f\" - Empty meta-features or target variables for {stock}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # ----- Train Meta-Model (Ridge Regression with Cross-Validation) -----\n",
    "    try:\n",
    "        meta_model = RidgeCV()\n",
    "        meta_model.fit(meta_features_train, y_train)\n",
    "        print(f\" - Meta-Model (Ridge Regression) trained successfully for {stock}.\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error training Meta-Model for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # ----- Save Meta-Model -----\n",
    "    meta_model_path = os.path.join(meta_model_dir, f'stacking_meta_model_{stock}.pkl')\n",
    "    try:\n",
    "        joblib.dump(meta_model, meta_model_path)\n",
    "        print(f\" - Meta-Model saved at '{meta_model_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error saving Meta-Model for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # ----- Generate Meta-Predictions on Test Data -----\n",
    "    try:\n",
    "        meta_pred_test = meta_model.predict(meta_features_test)\n",
    "        print(f\" - Meta-Predictions generated for {stock}.\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error generating Meta-Predictions for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # ----- Evaluate Meta-Model -----\n",
    "    try:\n",
    "        rmse_meta = np.sqrt(mean_squared_error(y_test, meta_pred_test))\n",
    "        mae_meta = mean_absolute_error(y_test, meta_pred_test)\n",
    "        r2_meta = r2_score(y_test, meta_pred_test)\n",
    "        mape_meta = mean_absolute_percentage_error(y_test, meta_pred_test) * 100\n",
    "        \n",
    "        meta_model_per_stock[stock] = {\n",
    "            'RMSE': rmse_meta,\n",
    "            'MAE': mae_meta,\n",
    "            'R2': r2_meta,\n",
    "            'MAPE': mape_meta\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nMeta-Model Evaluation Metrics for {stock}:\")\n",
    "        print(f\"    RMSE = {rmse_meta:.4f}\")\n",
    "        print(f\"    MAE = {mae_meta:.4f}\")\n",
    "        print(f\"    R2 = {r2_meta:.4f}\")\n",
    "        print(f\"    MAPE = {mape_meta:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error evaluating Meta-Model for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # ----- Create Evaluation DataFrame for Meta-Model -----\n",
    "    try:\n",
    "        # Align test_dates with predictions\n",
    "        # Extract dates similar to Cell 10\n",
    "        if isinstance(meta_features_test.index, pd.DatetimeIndex):\n",
    "            adjusted_test_dates = meta_features_test.index\n",
    "        elif 'Date' in meta_features_test.columns:\n",
    "            adjusted_test_dates = pd.to_datetime(meta_features_test['Date'], errors='coerce')\n",
    "        else:\n",
    "            adjusted_test_dates = pd.date_range(start='2020-01-01', periods=len(meta_pred_test), freq='D')\n",
    "        \n",
    "        eval_df_meta = pd.DataFrame({\n",
    "            'Date': adjusted_test_dates,\n",
    "            'Actual': y_test,\n",
    "            'Meta_Predicted': meta_pred_test\n",
    "        })\n",
    "        \n",
    "        # ----- Add Time Features -----\n",
    "        eval_df_meta['Month'] = eval_df_meta['Date'].dt.month\n",
    "        eval_df_meta['Quarter'] = eval_df_meta['Date'].dt.quarter\n",
    "        eval_df_meta['Season'] = eval_df_meta['Month'].apply(\n",
    "            lambda month: 'Winter' if month in [12, 1, 2] else\n",
    "                          'Spring' if month in [3, 4, 5] else\n",
    "                          'Summer' if month in [6, 7, 8] else\n",
    "                          'Autumn'\n",
    "        )\n",
    "        print(f\" - Time features added to the meta-model evaluation DataFrame for {stock}.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" - Error adding time features for {stock}: {e}\")\n",
    "    \n",
    "    # ----- Save Meta-Model Predictions -----\n",
    "    try:\n",
    "        meta_pred_save_path = os.path.join(meta_model_dir, f'meta_predictions_{stock}.csv')\n",
    "        eval_df_meta.to_csv(meta_pred_save_path, index=False)\n",
    "        print(f\" - Meta-Model predictions saved at '{meta_pred_save_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error saving Meta-Model predictions for {stock}: {e}\")\n",
    "    \n",
    "    # ----- Store Meta-Model Performance -----\n",
    "    print(f\" - Meta-Model performance metrics stored for {stock}.\")\n",
    "    \n",
    "    # ----- Detailed Descriptive Statistics -----\n",
    "    try:\n",
    "        df = pd.DataFrame({\n",
    "            'Actual': y_test,\n",
    "            'Predicted': meta_pred_test\n",
    "        })\n",
    "        descriptive_stats = df.describe().T\n",
    "        correlation = df.corr().loc['Actual', 'Predicted']\n",
    "        descriptive_stats['Correlation'] = correlation\n",
    "        print(f\"\\nDescriptive Statistics for {stock}:\")\n",
    "        display(descriptive_stats)\n",
    "        \n",
    "        # Save Descriptive Statistics\n",
    "        descriptive_stats.to_csv(os.path.join(meta_model_dir, f'detailed_metrics_{stock}.csv'))\n",
    "        print(f\" - Detailed metrics saved for {stock} at '{meta_model_dir}/detailed_metrics_{stock}.csv'\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error generating detailed statistics for {stock}: {e}\")\n",
    "\n",
    "# ----- Create Overall Metrics Table -----\n",
    "overall_metrics_meta_df = pd.DataFrame(meta_model_per_stock).T\n",
    "overall_metrics_meta_df = overall_metrics_meta_df[['RMSE', 'MAE', 'R2', 'MAPE']].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks - Meta Stacked Model\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_meta_df)\n",
    "overall_metrics_meta_df.to_csv('overall_evaluation_metrics_meta_stacked.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table for Meta Stacked Model saved as 'overall_evaluation_metrics_meta_stacked.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1493d1-a997-4702-9859-9b3b9a9bbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Define Essential Functions\n",
    "\n",
    "# Function to load all necessary models and scalers for a given stock\n",
    "def load_models(stock):\n",
    "    try:\n",
    "        # Define model directories\n",
    "        xgb_model_dir = '../models/xgb_models'\n",
    "        rf_model_dir = '../models/random_forest_models'\n",
    "        lstm_model_dir = '../models/lstm_models'\n",
    "        gru_model_dir = '../models/gru_models'\n",
    "        meta_model_dir = '../models/meta_model'\n",
    "        scalers_dir = '../models/scalers'\n",
    "        \n",
    "        # Load XGBoost Model\n",
    "        xgb_model_path = os.path.join(xgb_model_dir, f'xgb_{stock}_model.json')\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.load_model(xgb_model_path)\n",
    "        print(f\" - Loaded XGBoost model from '{xgb_model_path}'\")\n",
    "        \n",
    "        # Load Random Forest Model\n",
    "        rf_model_path = os.path.join(rf_model_dir, f'rf_{stock}_model.pkl')\n",
    "        rf_model = joblib.load(rf_model_path)\n",
    "        print(f\" - Loaded Random Forest model from '{rf_model_path}'\")\n",
    "        \n",
    "        # Load LSTM Model\n",
    "        lstm_model_path = os.path.join(lstm_model_dir, f'lstm_{stock}_best.keras')\n",
    "        lstm_model = load_model(lstm_model_path)\n",
    "        print(f\" - Loaded LSTM model from '{lstm_model_path}'\")\n",
    "        \n",
    "        # Load GRU Model\n",
    "        gru_model_path = os.path.join(gru_model_dir, f'gru_{stock}_best.keras')\n",
    "        gru_model = load_model(gru_model_path)\n",
    "        print(f\" - Loaded GRU model from '{gru_model_path}'\")\n",
    "        \n",
    "        # Load Meta-Model\n",
    "        meta_model_path = os.path.join(meta_model_dir, f'stacking_meta_model_{stock}.pkl')\n",
    "        meta_model = joblib.load(meta_model_path)\n",
    "        print(f\" - Loaded Meta-Model from '{meta_model_path}'\")\n",
    "        \n",
    "        # Load Scalers\n",
    "        scaler_X_path = os.path.join(scalers_dir, f'minmax_scaler_X_{stock}.joblib')\n",
    "        scaler_y_path = os.path.join(scalers_dir, f'minmax_scaler_y_{stock}.joblib')\n",
    "        scaler_X = joblib.load(scaler_X_path)\n",
    "        scaler_y = joblib.load(scaler_y_path)\n",
    "        print(f\" - Loaded Scalers from '{scaler_X_path}' and '{scaler_y_path}'\")\n",
    "        \n",
    "        return xgb_model, rf_model, lstm_model, gru_model, meta_model, scaler_X, scaler_y\n",
    "    except Exception as e:\n",
    "        print(f\" - Error loading models or scalers for {stock}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate features for the next day forecast\n",
    "def generate_next_day_features(current_data_unscaled, scaler_X, timesteps=60):\n",
    "    if len(current_data_unscaled) < timesteps:\n",
    "        raise ValueError(\"Insufficient data to generate features.\")\n",
    "    \n",
    "    latest_data = current_data_unscaled.tail(timesteps).copy()\n",
    "    if 'Close' not in latest_data.columns:\n",
    "        raise KeyError(\"'Close' column is missing in current_data_unscaled.\")\n",
    "\n",
    "    # Drop 'Close' and 'Date' to get feature columns\n",
    "    if 'Date' in latest_data.columns:\n",
    "        features_for_models = latest_data.drop(columns=['Close', 'Date'])\n",
    "    else:\n",
    "        features_for_models = latest_data.drop(columns=['Close'])\n",
    "    \n",
    "    # Ensure features are in the same order as during training\n",
    "    expected_features = scaler_X.feature_names_in_\n",
    "    missing_features = set(expected_features) - set(features_for_models.columns)\n",
    "    if missing_features:\n",
    "        raise KeyError(f\"Missing required columns: {missing_features}\")\n",
    "    \n",
    "    features_for_models = features_for_models[expected_features]\n",
    "    \n",
    "    # Scale the features\n",
    "    xgb_rf_features_scaled = scaler_X.transform(features_for_models.iloc[-1:].copy())\n",
    "    lstm_gru_features_scaled = scaler_X.transform(features_for_models).reshape(1, timesteps, -1)\n",
    "    \n",
    "    latest_features = features_for_models.iloc[-1].copy()\n",
    "    \n",
    "    return xgb_rf_features_scaled, lstm_gru_features_scaled, latest_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae03e0-1934-44d3-b2ca-19136e0116ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Initialize and Prepare Data\n",
    "\n",
    "# Define Paths\n",
    "raw_data_dir = '../data/stock_data'\n",
    "forecast_save_dir = '../models/future_forecasts'\n",
    "os.makedirs(forecast_save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a dictionary to store models and scaled data for each stock\n",
    "models_per_stock = {}\n",
    "timesteps = 60\n",
    "\n",
    "for stock in stocks:\n",
    "    print(f\"\\n{'='*50}\\nProcessing Stock: {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models(stock)\n",
    "    if models is None:\n",
    "        print(f\" - Skipping stock '{stock}' due to model loading issues.\")\n",
    "        continue\n",
    "    xgb_model, rf_model, lstm_model, gru_model, meta_model, scaler_X, scaler_y = models\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_csv_path = os.path.join(raw_data_dir, f\"{stock}_daily.csv\")\n",
    "    if not os.path.exists(raw_csv_path):\n",
    "        print(f\" - Raw data CSV not found at '{raw_csv_path}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(raw_csv_path)\n",
    "        print(f\" - Loaded raw data from '{raw_csv_path}'. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error reading CSV for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Ensure 'Date' column is present\n",
    "    if 'Date' not in df.columns:\n",
    "        print(f\" - 'Date' column missing in '{raw_csv_path}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert 'Date' to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isnull().any():\n",
    "        print(f\" - Some 'Date' entries could not be converted to datetime for {stock}. Dropping these rows.\")\n",
    "        df.dropna(subset=['Date'], inplace=True)\n",
    "    \n",
    "    # Sort by Date\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Add Enhanced 'Close' Price-Based Features\n",
    "    try:\n",
    "        df_fe = add_close_price_features(df)\n",
    "        print(f\" - Applied enhanced 'Close' price-based feature engineering. Shape: {df_fe.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error during feature engineering for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Keep only necessary recent data\n",
    "    current_data_unscaled = df_fe.tail(timesteps).copy()\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in current_data_unscaled.columns if col not in ['Date', 'Close']]\n",
    "    X_current = current_data_unscaled[feature_cols].copy()\n",
    "    \n",
    "    # Drop 'Date' column if present\n",
    "    if 'Date' in X_current.columns:\n",
    "        X_current = X_current.drop(columns=['Date'])\n",
    "        print(\" - Dropped 'Date' column from features.\")\n",
    "    \n",
    "    # Ensure features are in the same order as during training\n",
    "    expected_features = scaler_X.feature_names_in_\n",
    "    \n",
    "    # Identify missing features\n",
    "    missing_features = set(expected_features) - set(X_current.columns)\n",
    "    if missing_features:\n",
    "        print(f\" - Missing Features for {stock}: {missing_features}\")\n",
    "        # Add missing features with default values (e.g., 0)\n",
    "        for feature in missing_features:\n",
    "            X_current[feature] = 0  # Alternatively, use df_fe[feature].iloc[-1] or another strategy\n",
    "        print(f\" - Added missing features with default values for {stock}.\")\n",
    "    \n",
    "    # Reorder columns to match expected_features\n",
    "    X_current = X_current[expected_features]\n",
    "    \n",
    "    # Scale features and target\n",
    "    try:\n",
    "        X_current_scaled = pd.DataFrame(scaler_X.transform(X_current), columns=X_current.columns)\n",
    "        current_data_scaled = X_current_scaled.copy()\n",
    "        current_data_scaled['Close'] = scaler_y.transform(current_data_unscaled['Close'].values.reshape(-1, 1)).flatten()\n",
    "        current_data_scaled['Date'] = current_data_unscaled['Date'].values\n",
    "        print(f\" - Scaled current data for {stock}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error scaling data for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Store prepared data in models_per_stock\n",
    "    models_per_stock[stock] = {\n",
    "        'models': models,\n",
    "        'current_data_scaled': current_data_scaled,\n",
    "        'current_data_unscaled': current_data_unscaled,\n",
    "        'scaler_y': scaler_y,\n",
    "        # 'y_train_scaled': y_train_scaled,  # Uncomment if available\n",
    "        # 'y_test_scaled': y_test_scaled     # Uncomment if available\n",
    "    }\n",
    "    \n",
    "    print(f\" - Current data for {stock} loaded and prepared. Total samples: {len(current_data_scaled)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825e712-a801-4c36-9688-01848a88f671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: Forecasting Future Prices with Corrected Feature Engineering and pd.concat()\n",
    "\n",
    "def inverse_scale_prediction(scaler_y, prediction_scaled):\n",
    "    return scaler_y.inverse_transform([[prediction_scaled]])[0][0]\n",
    "\n",
    "# Define directories\n",
    "forecast_save_dir = '../models/future_forecasts'\n",
    "os.makedirs(forecast_save_dir, exist_ok=True)\n",
    "\n",
    "# Define forecast days\n",
    "forecast_days = 30\n",
    "\n",
    "# Initialize a dictionary to store forecast results\n",
    "forecast_results = {stock: [] for stock in models_per_stock.keys()}\n",
    "\n",
    "# Start forecasting\n",
    "for stock in models_per_stock.keys():\n",
    "    print(f\"\\nStarting Forecasting for {stock}\")\n",
    "    try:\n",
    "        models = models_per_stock[stock]['models']\n",
    "        current_data_scaled = models_per_stock[stock]['current_data_scaled']\n",
    "        current_data_unscaled = models_per_stock[stock]['current_data_unscaled']\n",
    "        scaler_X = models_per_stock[stock]['models'][5]  # scaler_X\n",
    "        scaler_y = models_per_stock[stock]['models'][6]  # scaler_y\n",
    "        \n",
    "        # Corrected model unpacking\n",
    "        xgb_model, rf_model, lstm_model, gru_model, meta_model, scaler_X, scaler_y = models\n",
    "        \n",
    "        # Create a copy of current_data_unscaled to update with predictions\n",
    "        updated_unscaled = current_data_unscaled.copy()\n",
    "        \n",
    "        for day in range(1, forecast_days + 1):\n",
    "            # Generate features for XGBoost and Random Forest\n",
    "            xgb_rf_features_scaled, lstm_gru_features_scaled, latest_features = generate_next_day_features(\n",
    "                updated_unscaled,\n",
    "                scaler_X,\n",
    "                timesteps=60\n",
    "            )\n",
    "            \n",
    "            # Make predictions with base models\n",
    "            xgb_pred_scaled = xgb_model.predict(xgb_rf_features_scaled)[0]\n",
    "            rf_pred_scaled = rf_model.predict(xgb_rf_features_scaled)[0]\n",
    "            lstm_pred_scaled = lstm_model.predict(lstm_gru_features_scaled)[0][0]\n",
    "            gru_pred_scaled = gru_model.predict(lstm_gru_features_scaled)[0][0]\n",
    "            \n",
    "            # Debug: Print scaled predictions\n",
    "            print(f\" - Day {day}:\")\n",
    "            print(f\"   XGBoost_scaled = {xgb_pred_scaled}\")\n",
    "            print(f\"   RF_scaled      = {rf_pred_scaled}\")\n",
    "            print(f\"   LSTM_scaled    = {lstm_pred_scaled}\")\n",
    "            print(f\"   GRU_scaled     = {gru_pred_scaled}\")\n",
    "            \n",
    "            # Meta-model prediction\n",
    "            meta_features = np.array([xgb_pred_scaled, rf_pred_scaled, lstm_pred_scaled, gru_pred_scaled]).reshape(1, -1)\n",
    "            meta_pred_scaled = meta_model.predict(meta_features)[0]\n",
    "            \n",
    "            # Debug: Print scaled meta-prediction\n",
    "            print(f\"   Meta-model_scaled_prediction = {meta_pred_scaled}\")\n",
    "            \n",
    "            # Inversely scale the meta-prediction\n",
    "            meta_pred = inverse_scale_prediction(scaler_y, meta_pred_scaled)\n",
    "            \n",
    "            # Debug: Print inversely scaled prediction\n",
    "            print(f\"   Meta-model_prediction (original scale) = {meta_pred}\")\n",
    "            \n",
    "            # Append the prediction\n",
    "            forecast_results[stock].append(meta_pred)\n",
    "            \n",
    "            # Update the unscaled data with the new prediction\n",
    "            # Assuming 'Close' is the target variable\n",
    "            new_row = latest_features.copy()\n",
    "            new_row['Close'] = meta_pred\n",
    "            new_row['Date'] = updated_unscaled['Date'].max() + BDay(1)  # Increment date by 1 business day\n",
    "            \n",
    "            # Debug: Print the new row before feature engineering\n",
    "            print(f\"   New row before feature engineering:\\n{new_row}\")\n",
    "            \n",
    "            # Convert new_row to DataFrame\n",
    "            new_row_df = pd.DataFrame([new_row])\n",
    "            \n",
    "            # Append the new row to updated_unscaled using pd.concat()\n",
    "            updated_unscaled = pd.concat([updated_unscaled, new_row_df], ignore_index=True)\n",
    "            \n",
    "            # Recalculate any derived features based on the new 'Close' price\n",
    "            updated_unscaled = add_close_price_features(updated_unscaled)\n",
    "            \n",
    "            # Ensure that only the last 'timesteps' rows are kept\n",
    "            updated_unscaled = updated_unscaled.tail(60).reset_index(drop=True)\n",
    "            \n",
    "            # Debug: Print the last few rows of updated_unscaled to verify updates\n",
    "            print(f\"   Updated 'Close' prices after Day {day}:\")\n",
    "            print(updated_unscaled['Close'].tail(5).values)\n",
    "        \n",
    "        # Verify the number of predictions\n",
    "        num_predictions = len(forecast_results[stock])\n",
    "        print(f\"{stock}: Number of predictions = {num_predictions}, Expected = {forecast_days}\")\n",
    "        \n",
    "        # Truncate excess predictions if any\n",
    "        if num_predictions > forecast_days:\n",
    "            print(f\"{stock}: Truncating {num_predictions - forecast_days} excess predictions.\")\n",
    "            forecast_results[stock] = forecast_results[stock][:forecast_days]\n",
    "        elif num_predictions < forecast_days:\n",
    "            print(f\"{stock}: Missing {forecast_days - num_predictions} predictions.\")\n",
    "            # Optionally, handle missing predictions\n",
    "            # For now, we'll skip saving forecasts for this stock\n",
    "            continue\n",
    "        \n",
    "        # Create Forecast DataFrame\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'Day': range(1, forecast_days + 1),\n",
    "            f'{stock}_Predicted': forecast_results[stock]\n",
    "        })\n",
    "        \n",
    "        # Save Forecast\n",
    "        forecast_save_path = os.path.join(forecast_save_dir, f'future_forecasts_{stock}.csv')\n",
    "        forecast_df.to_csv(forecast_save_path, index=False)\n",
    "        print(f\" - Forecast saved at '{forecast_save_path}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" - Error forecasting for {stock}: {e}\")\n",
    "\n",
    "# Display a sample of forecast_results\n",
    "for stock in models_per_stock.keys():\n",
    "    preds = forecast_results.get(stock, [])\n",
    "    if preds:\n",
    "        print(f\"\\nSample Predictions for {stock}: {preds[:5]} ...\")\n",
    "    else:\n",
    "        print(f\"\\nNo predictions available for {stock}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed971a0b-f6b9-4161-aca7-a8ae0b89a58a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 15: Loading Forecasts and Historical Data\n",
    "\n",
    "def load_forecast(stock, forecast_dir):\n",
    "    forecast_path = os.path.join(forecast_dir, f'future_forecasts_{stock}.csv')\n",
    "    if not os.path.exists(forecast_path):\n",
    "        print(f\" - Forecast file for {stock} not found at '{forecast_path}'.\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(forecast_path)\n",
    "        print(f\" - Loaded forecast for {stock} from '{forecast_path}'. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\" - Error loading forecast for {stock}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_historical_data(stock, raw_data_dir, validation_days=30):\n",
    "    historical_path = os.path.join(raw_data_dir, f\"{stock}_daily.csv\")\n",
    "    if not os.path.exists(historical_path):\n",
    "        print(f\" - Historical data for {stock} not found at '{historical_path}'.\")\n",
    "        return None, None\n",
    "    try:\n",
    "        df = pd.read_csv(historical_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df.dropna(subset=['Date'], inplace=True)\n",
    "        df.sort_values('Date', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        if len(df) < validation_days + 1:\n",
    "            print(f\" - Not enough data for {stock} to perform backtesting. Required: {validation_days + 1}, Available: {len(df)}\")\n",
    "            return None, None\n",
    "        \n",
    "        training_df = df.iloc[:-validation_days].copy()\n",
    "        validation_df = df.iloc[-validation_days:].copy()\n",
    "        \n",
    "        print(f\" - Loaded historical data for {stock} from '{historical_path}'. Training Shape: {training_df.shape}, Validation Shape: {validation_df.shape}\")\n",
    "        return training_df, validation_df\n",
    "    except Exception as e:\n",
    "        print(f\" - Error loading historical data for {stock}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Define directories\n",
    "forecast_dir = '../models/future_forecasts'\n",
    "raw_data_dir = '../data/stock_data'  # Ensure this path is correct relative to your notebook\n",
    "\n",
    "# List of stocks\n",
    "stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n",
    "\n",
    "# Initialize dictionaries to store forecasts and historical data\n",
    "forecasts = {}\n",
    "historicals = {}\n",
    "\n",
    "# Load forecasts and historical data\n",
    "for stock in stocks:\n",
    "    print(f\"\\nLoading data for {stock}:\")\n",
    "    forecasts[stock] = load_forecast(stock, forecast_dir)\n",
    "    historicals[stock] = load_historical_data(stock, raw_data_dir, validation_days=30)\n",
    "\n",
    "# Inspect loaded data (Optional but recommended)\n",
    "for stock in stocks:\n",
    "    print(f\"\\nInspecting data for {stock}:\")\n",
    "    forecast_df = forecasts.get(stock)\n",
    "    historical_data = historicals.get(stock)\n",
    "    \n",
    "    if historical_data is not None:\n",
    "        training_df, validation_df = historical_data\n",
    "    else:\n",
    "        training_df, validation_df = None, None\n",
    "    \n",
    "    if forecast_df is not None:\n",
    "        print(\"Forecast DataFrame Head:\")\n",
    "        print(forecast_df.head())\n",
    "    \n",
    "    if training_df is not None:\n",
    "        print(\"Training DataFrame Tail:\")\n",
    "        print(training_df.tail())\n",
    "    \n",
    "    if validation_df is not None:\n",
    "        print(\"Validation DataFrame Tail:\")\n",
    "        print(validation_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf8c12-266e-4643-ae69-bbef348c399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "# Define directories (adjust paths as necessary)\n",
    "forecast_dir = '../models/future_forecasts'\n",
    "raw_data_dir = '../data/stock_data'  # Ensure this path is correct relative to your notebook\n",
    "\n",
    "# Function to load forecasts\n",
    "def load_forecast(stock, forecast_dir):\n",
    "    forecast_path = os.path.join(forecast_dir, f'future_forecasts_{stock}.csv')\n",
    "    if not os.path.exists(forecast_path):\n",
    "        print(f\" - Forecast file for {stock} not found at '{forecast_path}'.\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(forecast_path)\n",
    "        print(f\" - Loaded forecast for {stock} from '{forecast_path}'. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\" - Error loading forecast for {stock}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load historical data\n",
    "def load_historical_data(stock, raw_data_dir, validation_days=30):\n",
    "    historical_path = os.path.join(raw_data_dir, f\"{stock}_daily.csv\")\n",
    "    if not os.path.exists(historical_path):\n",
    "        print(f\" - Historical data for {stock} not found at '{historical_path}'.\")\n",
    "        return None, None\n",
    "    try:\n",
    "        df = pd.read_csv(historical_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df.dropna(subset=['Date'], inplace=True)\n",
    "        df.sort_values('Date', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        if len(df) < validation_days + 1:\n",
    "            print(f\" - Not enough data for {stock} to perform backtesting. Required: {validation_days + 1}, Available: {len(df)}\")\n",
    "            return None, None\n",
    "        \n",
    "        training_df = df.iloc[:-validation_days].copy()\n",
    "        validation_df = df.iloc[-validation_days:].copy()\n",
    "        \n",
    "        print(f\" - Loaded historical data for {stock} from '{historical_path}'. Training Shape: {training_df.shape}, Validation Shape: {validation_df.shape}\")\n",
    "        return training_df, validation_df\n",
    "    except Exception as e:\n",
    "        print(f\" - Error loading historical data for {stock}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Initialize dictionaries to store forecasts and historical data\n",
    "forecasts = {}\n",
    "historicals = {}\n",
    "\n",
    "# Load forecasts and historical data\n",
    "for stock in stocks:\n",
    "    print(f\"\\nLoading data for {stock}:\")\n",
    "    forecasts[stock] = load_forecast(stock, forecast_dir)\n",
    "    historicals[stock] = load_historical_data(stock, raw_data_dir, validation_days=30)\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_rows', None)       # Display all rows\n",
    "pd.set_option('display.max_columns', None)    # Display all columns\n",
    "pd.set_option('display.width', None)          # No wrapping in output\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)  # Format floats\n",
    "\n",
    "# Plotting the forecasts alongside historical data\n",
    "for stock in stocks:\n",
    "    forecast_df = forecasts.get(stock)\n",
    "    historical_data = historicals.get(stock)\n",
    "    \n",
    "    if historical_data is not None and forecast_df is not None:\n",
    "        training_df, validation_df = historical_data\n",
    "        \n",
    "        # Combine training and validation data\n",
    "        historical_df = pd.concat([training_df, validation_df], ignore_index=True)\n",
    "        historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n",
    "        \n",
    "        # Get the last date from the historical data\n",
    "        last_historical_date = historical_df['Date'].max()\n",
    "        \n",
    "        # Generate future dates for forecasts\n",
    "        forecast_days = forecast_df.shape[0]\n",
    "        \n",
    "        # Use custom business days to account for weekends and US Federal Holidays\n",
    "        us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "        future_dates = pd.date_range(start=last_historical_date + pd.Timedelta(days=1), periods=forecast_days, freq=us_bd)\n",
    "        \n",
    "        # Add dates to forecast_df\n",
    "        forecast_df['Date'] = future_dates\n",
    "        forecast_df.rename(columns={f'{stock}_Predicted': 'Predicted_Close'}, inplace=True)\n",
    "        \n",
    "        # Merge historical and forecast data\n",
    "        combined_df = pd.merge(historical_df[['Date', 'Close']], forecast_df[['Date', 'Predicted_Close']], on='Date', how='outer')\n",
    "        combined_df.sort_values('Date', inplace=True)\n",
    "        combined_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Focus on the last year of data\n",
    "        one_year_ago = last_historical_date - pd.DateOffset(years=1)\n",
    "        mask = combined_df['Date'] >= one_year_ago\n",
    "        combined_df_last_year = combined_df.loc[mask].reset_index(drop=True)\n",
    "        \n",
    "        # Print out the combined DataFrame for the last year\n",
    "        print(f\"\\nCombined Data for {stock} - Last Year:\")\n",
    "        print(combined_df_last_year[['Date', 'Close', 'Predicted_Close']].to_string(index=False))\n",
    "        \n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(14,7))\n",
    "        plt.plot(combined_df_last_year['Date'], combined_df_last_year['Close'], label='Actual Close Prices')\n",
    "        plt.plot(combined_df_last_year['Date'], combined_df_last_year['Predicted_Close'], label='Predicted Close Prices', linestyle='--')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title(f'Stock Price Prediction for {stock} - Last Year')\n",
    "        plt.axvline(x=last_historical_date, color='grey', linestyle='--', label='Forecast Start')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Data not available for {stock}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec129f3-6638-49be-9035-c1ce30017181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d1254-93f5-4302-a871-c997cb7ffc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
