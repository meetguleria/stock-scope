{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0a790-3299-41b6-87b4-b191fd3efe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 : Standard Library Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import yfinance as yf\n",
    "import keras_tuner as kt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import ta\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import pandas_market_calendars as mcal\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Input, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, \n",
    "                             mean_absolute_percentage_error, r2_score)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import skew, kurtosis, shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67173a79-1f45-43c7-9ec7-82dddc594b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Fetch the Stock Data (Time-series Only)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize dictionaries to store dataframes\n",
    "daily_data_dict = {}\n",
    "\n",
    "# List of stocks to fetch data for\n",
    "stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n",
    "\n",
    "# Define the time frames for data\n",
    "end_date = datetime.now()\n",
    "start_date_daily = end_date - timedelta(days=10*365)   # 10 years of daily data\n",
    "\n",
    "# Create directories for the data\n",
    "os.makedirs('../data/stock_data', exist_ok=True)\n",
    "\n",
    "# Function to fetch stock data\n",
    "def fetch_stock_data(ticker, start, end, interval):\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start, end=end, interval=interval)\n",
    "        if data.empty:\n",
    "            logging.warning(f\"No data retrieved for {ticker} from {start} to {end} with interval {interval}\")\n",
    "        return data.drop(columns=['Adj Close'], errors='ignore')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch and save daily time-series data\n",
    "for stock in tqdm(stocks, desc=\"Fetching stocks data\"):\n",
    "\n",
    "    # Daily Data (10 years)\n",
    "    daily_data = fetch_stock_data(stock, start_date_daily, end_date, '1d')\n",
    "    if not daily_data.empty:\n",
    "        daily_data_dict[stock] = daily_data\n",
    "        daily_data.to_csv(f'../data/stock_data/{stock}_daily.csv', index=True)\n",
    "\n",
    "    # Add a delay to avoid API rate limits\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"Time-series data fetching and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb5640-0f73-4230-bff8-7416550f8dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 3A: Feature Engineering for Daily Data - Basic Technical Indicators\n",
    "\n",
    "# Define directories (assuming already defined)\n",
    "raw_data_dir = '../data/stock_data'\n",
    "\n",
    "# Initialize storage for enriched daily data\n",
    "enriched_daily_data_dict = {}\n",
    "\n",
    "# Feature Engineering Functions\n",
    "def add_basic_technical_indicators_daily(df):\n",
    "    # Simple and Exponential Moving Averages\n",
    "    df['SMA_20'] = ta.trend.SMAIndicator(close=df['Close'], window=20, fillna=False).sma_indicator()\n",
    "    df['EMA_20'] = ta.trend.EMAIndicator(close=df['Close'], window=20, fillna=False).ema_indicator()\n",
    "\n",
    "    # Relative Strength Index\n",
    "    df['RSI_14'] = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False).rsi()\n",
    "\n",
    "    # Moving Average Convergence Divergence\n",
    "    macd = ta.trend.MACD(close=df['Close'], fillna=False)\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_signal'] = macd.macd_signal()\n",
    "    df['MACD_diff'] = macd.macd_diff()\n",
    "\n",
    "    # Average True Range\n",
    "    df['ATR_14'] = ta.volatility.AverageTrueRange(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], window=14, fillna=False\n",
    "    ).average_true_range()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)\n",
    "    df['Bollinger_High'] = bollinger.bollinger_hband()\n",
    "    df['Bollinger_Low'] = bollinger.bollinger_lband()\n",
    "    df['Bollinger_Width'] = bollinger.bollinger_wband()\n",
    "    \n",
    "    # On-Balance Volume\n",
    "    df['OBV'] = ta.volume.OnBalanceVolumeIndicator(close=df['Close'], volume=df['Volume'], fillna=False).on_balance_volume()\n",
    "    \n",
    "    # Stochastic Oscillator\n",
    "    stoch = ta.momentum.StochasticOscillator(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], window=14, smooth_window=3, fillna=False\n",
    "    )\n",
    "    df['Stoch_%K'] = stoch.stoch()\n",
    "    df['Stoch_%D'] = stoch.stoch_signal()\n",
    "\n",
    "    # Rate of Change\n",
    "    df['ROC_10'] = ta.momentum.ROCIndicator(close=df['Close'], window=10, fillna=False).roc()\n",
    "    \n",
    "    # Daily Returns\n",
    "    df['Daily_Return'] = df['Close'].pct_change()\n",
    "    \n",
    "    # Close-Open Percent\n",
    "    df['Close_Open_Percent'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # High-Low Percent\n",
    "    df['High_Low_Percent'] = (df['High'] - df['Low']) / df['Low'] * 100\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_daily_stock_data_basic(stock):\n",
    "    daily_file = os.path.join(raw_data_dir, f\"{stock}_daily.csv\")\n",
    "    if not os.path.exists(daily_file):\n",
    "        logging.warning(f\"Daily data file not found for {stock}: {daily_file}\")\n",
    "        return\n",
    "\n",
    "    daily_df = pd.read_csv(daily_file, parse_dates=['Date'])\n",
    "    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    if not all(col in daily_df.columns for col in required_columns):\n",
    "        logging.error(f\"Missing required columns in {stock}'s data. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Ensure numeric data and handle missing values\n",
    "    daily_df[required_columns] = daily_df[required_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    daily_df.sort_values('Date', inplace=True)\n",
    "    daily_df.drop_duplicates(subset=['Date'], inplace=True)\n",
    "    daily_df.dropna(subset=required_columns, inplace=True)\n",
    "\n",
    "    # Apply Basic Technical Indicators\n",
    "    daily_df = add_basic_technical_indicators_daily(daily_df)\n",
    "\n",
    "    # Drop rows with NaN in critical features\n",
    "    critical_features = ['SMA_20', 'EMA_20', 'RSI_14', 'MACD', 'MACD_signal', 'MACD_diff', 'ATR_14',\n",
    "                         'Bollinger_High', 'Bollinger_Low', 'Bollinger_Width', 'OBV', 'Stoch_%K', 'Stoch_%D',\n",
    "                         'ROC_10', 'Daily_Return', 'Close_Open_Percent', 'High_Low_Percent']\n",
    "    daily_df.dropna(subset=critical_features, inplace=True)\n",
    "    \n",
    "    # Store the enriched DataFrame\n",
    "    enriched_daily_data_dict[stock] = daily_df\n",
    "    \n",
    "    # Logging\n",
    "    logging.info(f\"Basic technical indicators added for daily data of {stock}\")\n",
    "    logging.info(f\"Sample features:\\n{daily_df.head()}\")\n",
    "\n",
    "def verify_feature_engineered_data_basic(stock):\n",
    "    df = enriched_daily_data_dict.get(stock)\n",
    "    if df is None:\n",
    "        print(f\"No data found for {stock}.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Verifying data for {stock}...\")\n",
    "    \n",
    "    nan_counts = df.isna().sum()\n",
    "    missing = nan_counts[nan_counts > 0]\n",
    "    if not missing.empty:\n",
    "        print(f\"NaN counts:\\n{missing}\")\n",
    "        print(\"WARNING: There are still NaN values in the features.\")\n",
    "    else:\n",
    "        print(\"All features are free of NaN values.\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "def feature_engineer_daily_stock_data_basic(stock):\n",
    "    process_daily_stock_data_basic(stock)\n",
    "    verify_feature_engineered_data_basic(stock)\n",
    "\n",
    "# Process each stock's daily data\n",
    "for stock in daily_data_dict.keys():\n",
    "    feature_engineer_daily_stock_data_basic(stock)\n",
    "    \n",
    "print(\"Basic technical indicators added for all daily stock data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77dac94-6eeb-44c5-a306-2cc7b7fa07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3B: Feature Engineering for Daily Data - Advanced Features\n",
    "\n",
    "def calculate_vwap(df):\n",
    "    \"\"\"Calculate Volume Weighted Average Price (VWAP).\"\"\"\n",
    "    typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    vwap = (typical_price * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return vwap\n",
    "\n",
    "def add_advanced_features_daily(df):\n",
    "    # a. Relative Strength Index (RSI_14)\n",
    "    rsi = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False)\n",
    "    df['RSI_14'] = rsi.rsi()\n",
    "\n",
    "    # b. Moving Average Convergence Divergence (MACD)\n",
    "    macd = ta.trend.MACD(close=df['Close'], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_diff'] = macd.macd_diff()\n",
    "\n",
    "    # c. Average True Range (ATR_14)\n",
    "    atr = ta.volatility.AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close'], window=14, fillna=False)\n",
    "    df['ATR_14'] = atr.average_true_range()\n",
    "\n",
    "    # d. Bollinger Bands Width (Bollinger_Width)\n",
    "    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)\n",
    "    df['Bollinger_High'] = bollinger.bollinger_hband()\n",
    "    df['Bollinger_Low'] = bollinger.bollinger_lband()\n",
    "    df['Bollinger_Width'] = bollinger.bollinger_wband()\n",
    "\n",
    "    # e. Keltner Channels\n",
    "    keltner = ta.volatility.KeltnerChannel(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], window=20, window_atr=10, fillna=False\n",
    "    )\n",
    "    df['Keltner_High'] = keltner.keltner_channel_hband()\n",
    "    df['Keltner_Low'] = keltner.keltner_channel_lband()\n",
    "    df['Keltner_Width'] = df['Keltner_High'] - df['Keltner_Low']\n",
    "\n",
    "    # f. Chaikin Money Flow (CMF)\n",
    "    cmf_indicator = ta.volume.ChaikinMoneyFlowIndicator(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume'], window=20, fillna=False\n",
    "    )\n",
    "    df['CMF'] = cmf_indicator.chaikin_money_flow()\n",
    "\n",
    "    # 2. Lag Features\n",
    "    for lag in range(1, 4):\n",
    "        df[f'Lag_Close_{lag}'] = df['Close'].shift(lag)\n",
    "\n",
    "    # 3. Moving Averages and Standard Deviations\n",
    "    df['MA_5'] = df['Close'].rolling(window=5, min_periods=5).mean()\n",
    "    df['MA_Volume_10'] = df['Volume'].rolling(window=10, min_periods=10).mean()\n",
    "    df['STD_Close_5'] = df['Close'].rolling(window=5, min_periods=5).std()\n",
    "    df['MA_RSI_10'] = df['RSI_14'].rolling(window=10, min_periods=10).mean()\n",
    "    df['MA_MACD_10'] = df['MACD'].rolling(window=10, min_periods=10).mean()\n",
    "    df['MA_ATR_10'] = df['ATR_14'].rolling(window=10, min_periods=10).mean()\n",
    "    df['MA_Bollinger_Width_10'] = df['Bollinger_Width'].rolling(window=10, min_periods=10).mean()\n",
    "\n",
    "    # 4. VWAP and its Moving Average\n",
    "    df['VWAP'] = calculate_vwap(df)\n",
    "    df['MA_VWAP_10'] = df['VWAP'].rolling(window=10, min_periods=10).mean()\n",
    "\n",
    "    # 5. Date Features\n",
    "    if 'Date' not in df.columns:\n",
    "        raise KeyError(\"Missing 'Date' column required for date-based feature engineering.\")\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "\n",
    "    # 6. Rolling Correlations and Momentum\n",
    "    df['Rolling_Corr_Close_Volume_10'] = df['Close'].rolling(window=10, min_periods=10).corr(df['Volume'])\n",
    "    df['Momentum_5'] = df['Close'].diff(5)\n",
    "    df['Volatility_10'] = df['Close'].rolling(window=10, min_periods=10).std()\n",
    "\n",
    "    # 7. Cyclical Encoding for Day of Week and Month\n",
    "    df['Day_of_Week_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "    df['Day_of_Week_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    # Drop original Day_of_Week and Month\n",
    "    df.drop(['Day_of_Week', 'Month'], axis=1, inplace=True)\n",
    "\n",
    "    # 8. Interaction Features\n",
    "    df['MACD_diff_RSI_14'] = df['MACD_diff'] * df['RSI_14']\n",
    "    df['MACD_diff_ATR_14'] = df['MACD_diff'] * df['ATR_14']\n",
    "    df['RSI_14_ATR_14'] = df['RSI_14'] * df['ATR_14']\n",
    "\n",
    "    # 9. Trend and Seasonal Components\n",
    "    df['Trend'] = df['Close'].rolling(window=30, min_periods=30).mean()\n",
    "    df['Seasonal'] = df['Close'] - df['Trend']\n",
    "\n",
    "    # 10. Rolling Skewness and Kurtosis\n",
    "    df['Rolling_Skew_Close_10'] = df['Close'].rolling(window=10, min_periods=10).skew()\n",
    "    df['Rolling_Kurt_Close_10'] = df['Close'].rolling(window=10, min_periods=10).kurt()\n",
    "\n",
    "    # Fill any remaining NaN values\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_daily_stock_data_advanced(stock):\n",
    "    \"\"\"Process and add advanced features to a single stock's DataFrame.\"\"\"\n",
    "    df = enriched_daily_data_dict.get(stock)\n",
    "    if df is None:\n",
    "        logging.warning(f\"No basic technical indicator data found for {stock}. Skipping advanced feature engineering.\")\n",
    "        return\n",
    "    \n",
    "    # Apply Advanced Feature Engineering\n",
    "    df = add_advanced_features_daily(df)\n",
    "    \n",
    "    # Define critical advanced features to check for NaNs\n",
    "    advanced_features = [\n",
    "        'Lag_Close_1', 'Lag_Close_2', 'Lag_Close_3',\n",
    "        'MA_5', 'MA_Volume_10', 'STD_Close_5',\n",
    "        'MA_RSI_10', 'MA_MACD_10', 'MA_ATR_10', 'MA_Bollinger_Width_10',\n",
    "        'VWAP', 'MA_VWAP_10',\n",
    "        'Rolling_Corr_Close_Volume_10', 'Momentum_5', 'Volatility_10',\n",
    "        'Day_of_Week_sin', 'Day_of_Week_cos', 'Month_sin', 'Month_cos',\n",
    "        'MACD_diff_RSI_14', 'MACD_diff_ATR_14', 'RSI_14_ATR_14',\n",
    "        'Trend', 'Seasonal',\n",
    "        'Keltner_High', 'Keltner_Low', 'Keltner_Width',\n",
    "        'CMF',\n",
    "        'Rolling_Skew_Close_10', 'Rolling_Kurt_Close_10'\n",
    "    ]\n",
    "    \n",
    "    # Drop rows with NaN in advanced features\n",
    "    df.dropna(subset=advanced_features, inplace=True)\n",
    "    \n",
    "    # Store the updated DataFrame back into the dictionary\n",
    "    enriched_daily_data_dict[stock] = df\n",
    "    \n",
    "    # Logging\n",
    "    logging.info(f\"Advanced features added for daily data of {stock}\")\n",
    "    logging.info(f\"Sample advanced features for {stock}:\\n{df.head()}\")\n",
    "\n",
    "def verify_feature_engineered_data_advanced(stock):\n",
    "    \"\"\"Verify that advanced features are free of NaN values.\"\"\"\n",
    "    df = enriched_daily_data_dict.get(stock)\n",
    "    if df is None:\n",
    "        print(f\"No data found for {stock}.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Verifying advanced features for {stock}...\")\n",
    "    \n",
    "    nan_counts = df.isna().sum()\n",
    "    missing = nan_counts[nan_counts > 0]\n",
    "    if not missing.empty:\n",
    "        print(f\"NaN counts in advanced features:\\n{missing}\")\n",
    "        print(\"WARNING: There are still NaN values in the advanced features.\")\n",
    "    else:\n",
    "        print(\"All advanced features are free of NaN values.\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "def feature_engineer_daily_stock_data_advanced(stock):\n",
    "    \"\"\"Wrapper function to process and verify advanced features for a stock.\"\"\"\n",
    "    process_daily_stock_data_advanced(stock)\n",
    "    verify_feature_engineered_data_advanced(stock)\n",
    "\n",
    "# Process each stock's daily data for advanced features\n",
    "for stock in enriched_daily_data_dict.keys():\n",
    "    feature_engineer_daily_stock_data_advanced(stock)\n",
    "    \n",
    "print(\"Advanced features added for all daily stock data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511950f3-3177-4ab3-a9fb-299b9ab837e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Splitting Daily Data into Training and Testing Sets\n",
    "\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Function to split time series data\n",
    "def split_time_series_data(df, date_column, target_column, split_ratio=0.8):\n",
    "    # Sort the DataFrame by the date/time column\n",
    "    df_sorted = df.sort_values(by=date_column).reset_index(drop=True)\n",
    "    \n",
    "    # Determine the split index\n",
    "    split_index = int(len(df_sorted) * split_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_df = df_sorted.iloc[:split_index]\n",
    "    test_df = df_sorted.iloc[split_index:]\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop(columns=[target_column, date_column])\n",
    "    y_train = train_df[target_column]\n",
    "    \n",
    "    X_test = test_df.drop(columns=[target_column, date_column])\n",
    "    y_test = test_df[target_column]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Function to split the data for each stock\n",
    "def split_daily_data_dict(data_dict, date_column, target_column, split_ratio=0.8):\n",
    "    split_dict = {}\n",
    "    \n",
    "    for stock, df in data_dict.items():\n",
    "        logging.info(f\"Splitting data for {stock}...\")\n",
    "        X_train, X_test, y_train, y_test = split_time_series_data(df, date_column, target_column, split_ratio)\n",
    "        split_dict[stock] = {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        logging.info(f\"Completed splitting for {stock}.\")\n",
    "    \n",
    "    return split_dict\n",
    "\n",
    "# Function to verify the split data\n",
    "def verify_split(split_dict, stocks):\n",
    "    for stock in stocks:\n",
    "        data = split_dict.get(stock, {})\n",
    "        if not data:\n",
    "            print(f\"No split data found for {stock}.\")\n",
    "            continue\n",
    "        \n",
    "        X_train = data.get('X_train')\n",
    "        X_test = data.get('X_test')\n",
    "        y_train = data.get('y_train')\n",
    "        y_test = data.get('y_test')\n",
    "        \n",
    "        print(f\"Verifying split for {stock}:\")\n",
    "        print(f\" - Training set size: {X_train.shape[0]} samples\")\n",
    "        print(f\" - Testing set size: {X_test.shape[0]} samples\")\n",
    "        print(f\" - Feature columns in Training set: {X_train.columns.tolist()}\")\n",
    "        print(f\" - Feature columns in Testing set: {X_test.columns.tolist()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Define parameters\n",
    "daily_date_column = 'Date'    # Updated to match your data\n",
    "daily_target_column = 'Close' # Updated to match your data\n",
    "\n",
    "# Split Daily Data\n",
    "logging.info(\"Starting to split Daily Data...\")\n",
    "split_daily_data = split_daily_data_dict(\n",
    "    enriched_daily_data_dict,\n",
    "    date_column=daily_date_column,\n",
    "    target_column=daily_target_column,\n",
    "    split_ratio=0.8\n",
    ")\n",
    "logging.info(\"Completed splitting Daily Data.\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nVerifying Split for Daily Data:\")\n",
    "verify_split(split_daily_data, enriched_daily_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f740aa-6edf-4a11-a4be-e704ad21db5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Scaling the Data Using MinMaxScaler (Including Targets)\n",
    "\n",
    "# Function to scale data using MinMaxScaler for both features and targets\n",
    "def scale_data_with_target(split_data_dict):\n",
    "    scaled_data_dict = {}\n",
    "    \n",
    "    # Directories to save scalers\n",
    "    scaler_save_dir = '../models/scalers'\n",
    "    os.makedirs(scaler_save_dir, exist_ok=True)\n",
    "    \n",
    "    for stock, data in split_data_dict.items():\n",
    "        logging.info(f\"Scaling data for {stock}...\")\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        X_test = data['X_test']\n",
    "        y_train = data['y_train'].values.reshape(-1, 1)  # Reshape for scaler\n",
    "        y_test = data['y_test'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Initialize scalers\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        \n",
    "        # Fit scalers on training data and transform both training and testing data\n",
    "        X_train_scaled = pd.DataFrame(scaler_X.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(scaler_X.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "        \n",
    "        y_train_scaled = scaler_y.fit_transform(y_train).flatten()  # Flatten to 1D array\n",
    "        y_test_scaled = scaler_y.transform(y_test).flatten()\n",
    "        \n",
    "        # Save scalers using joblib\n",
    "        scaler_X_path = os.path.join(scaler_save_dir, f'minmax_scaler_X_{stock}.joblib')\n",
    "        scaler_y_path = os.path.join(scaler_save_dir, f'minmax_scaler_y_{stock}.joblib')\n",
    "        joblib.dump(scaler_X, scaler_X_path)\n",
    "        joblib.dump(scaler_y, scaler_y_path)\n",
    "        logging.info(f\"Scalers saved for {stock} at {scaler_X_path} and {scaler_y_path}.\")\n",
    "        \n",
    "        # Update the scaled data dictionary\n",
    "        scaled_data_dict[stock] = {\n",
    "            'X_train_scaled': X_train_scaled,\n",
    "            'X_test_scaled': X_test_scaled,\n",
    "            'y_train_scaled': y_train_scaled,\n",
    "            'y_test_scaled': y_test_scaled,\n",
    "            'scaler_X': scaler_X,\n",
    "            'scaler_y': scaler_y\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Completed scaling for {stock}.\")\n",
    "    \n",
    "    return scaled_data_dict\n",
    "\n",
    "# Scale the split daily data with target\n",
    "logging.info(\"Starting to scale Daily Data with target...\")\n",
    "scaled_daily_data = scale_data_with_target(split_daily_data)\n",
    "logging.info(\"Completed scaling Daily Data with target.\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nVerifying Scaled Data for Daily Data (Including Targets):\")\n",
    "for stock in scaled_daily_data.keys():\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    \n",
    "    print(f\"Scaled data for {stock}:\")\n",
    "    print(f\" - Scaled Training set shape: {X_train_scaled.shape}, Scaled Training targets shape: {y_train_scaled.shape}\")\n",
    "    print(f\" - Scaled Testing set shape: {X_test_scaled.shape}, Scaled Testing targets shape: {y_test_scaled.shape}\")\n",
    "    print(f\" - Feature columns: {X_train_scaled.columns.tolist()}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b973a-a31f-49e0-a6c0-e30aa9181a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Training and Evaluating LSTM Models for Daily Data\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Define Parameters\n",
    "TIMESTEPS = 60  # Number of past days to use for prediction\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # Increased to allow more training\n",
    "VALIDATION_SPLIT = 0.1  # Fraction of training data to use for validation\n",
    "\n",
    "# Define Evaluation Metrics Function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Function to Create Sequences\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to Build LSTM Model\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape)) # Explicit Input Layer\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Initialize a dictionary to store model performance\n",
    "model_performance = {}\n",
    "\n",
    "# Directories to save models and scalers\n",
    "model_save_dir = '../models/lstm_models'\n",
    "scaler_save_dir = '../models/scalers'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "os.makedirs(scaler_save_dir, exist_ok=True)\n",
    "\n",
    "# Iterate Through Each Stock\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining LSTM Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_X = data['scaler_X']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Create Sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    print(f\" - Training sequences: {X_train_seq.shape}, Training targets: {y_train_seq.shape}\")\n",
    "    print(f\" - Testing sequences: {X_test_seq.shape}, Testing targets: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Build the Model\n",
    "    model = build_lstm_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
    "    model.summary()\n",
    "    \n",
    "    # Define Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(model_save_dir, f'lstm_{stock}_best.keras'),  # Changed to .keras\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the Model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the Best Model\n",
    "    best_model_path = os.path.join(model_save_dir, f'lstm_{stock}_best.keras')\n",
    "    model = load_model(best_model_path)\n",
    "    print(f\" - Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = model.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Ensure Consistent Lengths\n",
    "    print(f\" - Length of y_test: {len(y_test)}\")\n",
    "    print(f\" - Length of predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    rmse, mae, r2 = evaluate_model(y_test, predictions)\n",
    "    model_performance[stock] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "    \n",
    "    print(f\" - Evaluation Metrics for {stock}: RMSE = {rmse:.4f}, MAE = {mae:.4f}, R2 = {r2:.4f}\")\n",
    "    \n",
    "    print(f\"Model training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Summary of Model Performance\n",
    "print(f\"\\n{'='*50}\\nSummary of Model Performance\\n{'='*50}\")\n",
    "for stock, metrics in model_performance.items():\n",
    "    print(f\"{stock}: RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}, R2 = {metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ffe6f-8de0-4578-a46f-eae51b249e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Evaluation and Plotting of LSTM Models with Numerical Metrics Tables\n",
    "\n",
    "TIMESTEPS = 60\n",
    "model_save_dir = '../models/lstm_models'\n",
    "\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def calculate_group_metrics(eval_df, group_by):\n",
    "    grouped = eval_df.groupby(group_by).apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    return grouped\n",
    "\n",
    "def plot_metrics_by_group(grouped_metrics, stock, grouping, metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=grouped_metrics.index, y=grouped_metrics[metric])\n",
    "    plt.title(f'{metric} by {grouping} for {stock}')\n",
    "    plt.xlabel(grouping)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_comparative_metrics(grouped_metrics, stocks, grouping, metric):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for stock in stocks:\n",
    "        sns.lineplot(data=grouped_metrics[stock][grouping], x=grouped_metrics[stock][grouping].index, y=metric, label=stock)\n",
    "    plt.title(f'Comparison of {metric} across {grouping}')\n",
    "    plt.xlabel(grouping)\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title='Stock')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def add_time_features(eval_df):\n",
    "    eval_df['Month'] = eval_df['Date'].dt.month\n",
    "    eval_df['Quarter'] = eval_df['Date'].dt.quarter\n",
    "    eval_df['Season'] = eval_df['Month'].apply(lambda month: 'Winter' if month in [12, 1, 2] else 'Spring' if month in [3,4,5] else 'Summer' if month in [6,7,8] else 'Autumn')\n",
    "    return eval_df\n",
    "\n",
    "overall_metrics = {}\n",
    "grouped_metrics_all = {'Month': {}, 'Quarter': {}, 'Season': {}}\n",
    "\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nEvaluating and Plotting LSTM Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    data = scaled_daily_data[stock]\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    best_model_path = os.path.join(model_save_dir, f'lstm_{stock}_best.keras')\n",
    "    if not os.path.exists(best_model_path):\n",
    "        print(f\" - Best model for {stock} not found at {best_model_path}. Skipping...\")\n",
    "        continue\n",
    "    try:\n",
    "        model = load_model(best_model_path)\n",
    "    except Exception as e:\n",
    "        print(f\" - Failed to load model for {stock}: {e}. Skipping...\")\n",
    "        continue\n",
    "    print(f\" - Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    predictions_scaled = model.predict(X_test_seq).flatten()\n",
    "    \n",
    "    try:\n",
    "        predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "        y_test = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    except Exception as e:\n",
    "        print(f\" - Failed to inverse transform predictions or actuals for {stock}: {e}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    if isinstance(X_test_scaled, pd.DataFrame) and isinstance(X_test_scaled.index, pd.DatetimeIndex):\n",
    "        dates = X_test_scaled.index[TIMESTEPS:]\n",
    "    elif isinstance(X_test_scaled, pd.DataFrame) and 'Date' in X_test_scaled.columns:\n",
    "        dates = pd.to_datetime(X_test_scaled['Date'].iloc[TIMESTEPS:])\n",
    "    else:\n",
    "        print(f\" - No Date information found for {stock}. Creating dummy dates.\")\n",
    "        dates = pd.date_range(start='2020-01-01', periods=len(y_test), freq='D')\n",
    "    \n",
    "    eval_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    \n",
    "    eval_df = add_time_features(eval_df)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(eval_df['Actual'], eval_df['Predicted']))\n",
    "    mae = mean_absolute_error(eval_df['Actual'], eval_df['Predicted'])\n",
    "    r2 = r2_score(eval_df['Actual'], eval_df['Predicted'])\n",
    "    mape = mean_absolute_percentage_error(eval_df['Actual'], eval_df['Predicted']) * 100\n",
    "    \n",
    "    overall_metrics[stock] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(f\" - Overall Evaluation Metrics for {stock}:\")\n",
    "    print(f\"    RMSE = {rmse:.4f}\")\n",
    "    print(f\"    MAE = {mae:.4f}\")\n",
    "    print(f\"    R2 = {r2:.4f}\")\n",
    "    print(f\"    MAPE = {mape:.2f}%\")\n",
    "    \n",
    "    grouped_metrics_month = calculate_group_metrics(eval_df, 'Month')\n",
    "    grouped_metrics_quarter = calculate_group_metrics(eval_df, 'Quarter')\n",
    "    grouped_metrics_season = calculate_group_metrics(eval_df, 'Season')\n",
    "    \n",
    "    grouped_metrics_all['Month'][stock] = grouped_metrics_month\n",
    "    grouped_metrics_all['Quarter'][stock] = grouped_metrics_quarter\n",
    "    grouped_metrics_all['Season'][stock] = grouped_metrics_season\n",
    "    \n",
    "    print(f\" - Plotting RMSE by Season for {stock}\")\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'RMSE')\n",
    "    \n",
    "    print(f\" - Plotting MAE by Season for {stock}\")\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'MAE')\n",
    "    \n",
    "    print(f\" - Plotting R2 by Season for {stock}\")\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'R2')\n",
    "    \n",
    "    print(f\" - Plotting MAPE by Season for {stock}\")\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'MAPE')\n",
    "    \n",
    "    print(f\"Evaluation and plotting completed for {stock}.\\n\")\n",
    "\n",
    "overall_metrics_df = pd.DataFrame(overall_metrics).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_df)\n",
    "overall_metrics_df.to_csv('overall_evaluation_metrics.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table saved as 'overall_evaluation_metrics.csv'.\")\n",
    "\n",
    "def create_grouped_metrics_tables(grouped_metrics_all, grouping):\n",
    "    grouped_metrics_tables = {}\n",
    "    for stock, metrics in grouped_metrics_all[grouping].items():\n",
    "        metrics_df = metrics.reset_index()\n",
    "        metrics_df.columns = [grouping] + list(metrics_df.columns[1:])\n",
    "        grouped_metrics_tables[stock] = metrics_df\n",
    "    return grouped_metrics_tables\n",
    "\n",
    "for grouping in ['Month', 'Quarter', 'Season']:\n",
    "    grouped_tables = create_grouped_metrics_tables(grouped_metrics_all, grouping)\n",
    "    for stock, table in grouped_tables.items():\n",
    "        print(f\"\\n{'='*50}\\n{grouping} Evaluation Metrics for {stock}\\n{'='*50}\")\n",
    "        display(table)\n",
    "        filename = f'{stock}_{grouping}_evaluation_metrics.csv'\n",
    "        table.to_csv(filename, index=False)\n",
    "        print(f\" - {grouping} Evaluation Metrics table for {stock} saved as '{filename}'.\")\n",
    "\n",
    "for grouping in ['Month', 'Quarter', 'Season']:\n",
    "    for metric in ['RMSE', 'MAE', 'R2', 'MAPE']:\n",
    "        comparative_df = pd.DataFrame({stock: grouped_metrics_all[grouping][stock][metric] for stock in grouped_metrics_all[grouping].keys()})\n",
    "        comparative_df.index.name = grouping\n",
    "        print(f\"\\n{'='*50}\\nComparative {metric} Across {grouping} for All Stocks\\n{'='*50}\")\n",
    "        display(comparative_df)\n",
    "        filename = f'comparative_{metric}_across_{grouping}.csv'\n",
    "        comparative_df.to_csv(filename)\n",
    "        print(f\" - Comparative {metric} Across {grouping} table saved as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720ab5c-ae48-4e11-842b-2bcf7da5c7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Training and Evaluating GRU Models for Daily Data\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dropout, Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define Parameters\n",
    "TIMESTEPS = 60  # Number of past days to use for prediction\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Function to Create Sequences (Already Defined in Cell 6)\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to Build GRU Model\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(GRU(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=25, activation='relu'))\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Initialize a dictionary to store GRU model performance\n",
    "gru_model_performance = {}\n",
    "\n",
    "# Define Directory to Save GRU Models\n",
    "gru_model_save_dir = '../models/gru_models'\n",
    "os.makedirs(gru_model_save_dir, exist_ok=True)\n",
    "\n",
    "# Iterate Through Each Stock\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining GRU Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Create Sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    print(f\" - Training sequences: {X_train_seq.shape}, Training targets: {y_train_seq.shape}\")\n",
    "    print(f\" - Testing sequences: {X_test_seq.shape}, Testing targets: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Build the GRU Model\n",
    "    model = build_gru_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
    "    model.summary()\n",
    "    \n",
    "    # Define Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(gru_model_save_dir, f'gru_{stock}_best.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the GRU Model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the Best Model\n",
    "    best_model_path = os.path.join(gru_model_save_dir, f'gru_{stock}_best.keras')\n",
    "    if os.path.exists(best_model_path):\n",
    "        model = load_model(best_model_path)\n",
    "        print(f\" - Loaded best model from {best_model_path}\")\n",
    "    else:\n",
    "        print(f\" - Best GRU model for {stock} not found at {best_model_path}.\")\n",
    "        continue  # Skip evaluation if model not saved\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = model.predict(X_test_seq).flatten()\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Ensure Consistent Lengths\n",
    "    print(f\" - Length of y_test: {len(y_test)}\")\n",
    "    print(f\" - Length of predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    gru_model_performance[stock] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "    \n",
    "    print(f\" - Evaluation Metrics for GRU {stock}: RMSE = {rmse:.4f}, MAE = {mae:.4f}, R2 = {r2:.4f}\")\n",
    "    \n",
    "    print(f\"GRU model training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Summary of GRU Model Performance\n",
    "print(f\"\\n{'='*50}\\nSummary of GRU Model Performance\\n{'='*50}\")\n",
    "for stock, metrics in gru_model_performance.items():\n",
    "    print(f\"{stock}: RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}, R2 = {metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bb105-329c-443d-a3a0-bb1bd94f072d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Training and Evaluating XGBoost Models\n",
    "\n",
    "# Define Parameters\n",
    "TIMESTEPS = 60  # Ensure consistency with LSTM\n",
    "model_save_dir = '../models/xgb_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "overall_metrics_xgb = {}\n",
    "grouped_metrics_all_xgb = {'Month': {}, 'Quarter': {}, 'Season': {}}\n",
    "\n",
    "# Function to Add Time Features\n",
    "def add_time_features_xgb(eval_df):\n",
    "    eval_df['Month'] = eval_df['Date'].dt.month\n",
    "    eval_df['Quarter'] = eval_df['Date'].dt.quarter\n",
    "    eval_df['Season'] = eval_df['Month'].apply(\n",
    "        lambda month: 'Winter' if month in [12, 1, 2] else\n",
    "                      'Spring' if month in [3, 4, 5] else\n",
    "                      'Summer' if month in [6, 7, 8] else\n",
    "                      'Autumn'\n",
    "    )\n",
    "    return eval_df\n",
    "\n",
    "# Iterate Through Each Stock for Evaluation and Plotting\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining and Evaluating XGBoost Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "\n",
    "    # Initialize and Train XGBoost Regressor\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train_scaled, y_train_scaled)\n",
    "    print(f\" - XGBoost model trained for {stock}\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create Evaluation DataFrame\n",
    "    if isinstance(X_test_scaled, pd.DataFrame) and isinstance(X_test_scaled.index, pd.DatetimeIndex):\n",
    "        dates = X_test_scaled.index\n",
    "    elif isinstance(X_test_scaled, pd.DataFrame) and 'Date' in X_test_scaled.columns:\n",
    "        dates = pd.to_datetime(X_test_scaled['Date'])\n",
    "    else:\n",
    "        print(f\" - No Date information found for {stock}. Creating dummy dates.\")\n",
    "        dates = pd.date_range(start='2020-01-01', periods=len(y_test), freq='D')\n",
    "    \n",
    "    eval_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    \n",
    "    # Add Time Features\n",
    "    eval_df = add_time_features_xgb(eval_df)\n",
    "    \n",
    "    # Calculate Overall Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df['Actual'], eval_df['Predicted']))\n",
    "    mae = mean_absolute_error(eval_df['Actual'], eval_df['Predicted'])\n",
    "    r2 = r2_score(eval_df['Actual'], eval_df['Predicted'])\n",
    "    mape = mean_absolute_percentage_error(eval_df['Actual'], eval_df['Predicted']) * 100\n",
    "    \n",
    "    overall_metrics_xgb[stock] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(f\" - Overall Evaluation Metrics for {stock}:\")\n",
    "    print(f\"    RMSE = {rmse:.4f}\")\n",
    "    print(f\"    MAE = {mae:.4f}\")\n",
    "    print(f\"    R2 = {r2:.4f}\")\n",
    "    print(f\"    MAPE = {mape:.2f}%\")\n",
    "    \n",
    "    # Calculate Grouped Metrics\n",
    "    grouped_metrics_month = eval_df.groupby('Month').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_quarter = eval_df.groupby('Quarter').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_season = eval_df.groupby('Season').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_all_xgb['Month'][stock] = grouped_metrics_month\n",
    "    grouped_metrics_all_xgb['Quarter'][stock] = grouped_metrics_quarter\n",
    "    grouped_metrics_all_xgb['Season'][stock] = grouped_metrics_season\n",
    "    \n",
    "    # Plot Metrics by Season\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'RMSE')\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'MAE')\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'R2')\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'MAPE')\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_save_path = os.path.join(model_save_dir, f'xgb_{stock}_model.json')\n",
    "    xgb_model.save_model(model_save_path)\n",
    "    print(f\" - XGBoost model saved for {stock} at {model_save_path}\")\n",
    "    \n",
    "    print(f\"Training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Create Overall Metrics Table\n",
    "overall_metrics_xgb_df = pd.DataFrame(overall_metrics_xgb).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks - XGBoost\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_xgb_df)\n",
    "overall_metrics_xgb_df.to_csv('overall_evaluation_metrics_xgb.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table for XGBoost saved as 'overall_evaluation_metrics_xgb.csv'.\")\n",
    "\n",
    "# Function to Create Grouped Metrics Tables\n",
    "def create_grouped_metrics_tables_xgb(grouped_metrics_all_xgb, grouping):\n",
    "    grouped_metrics_tables_xgb = {}\n",
    "    for stock, metrics in grouped_metrics_all_xgb[grouping].items():\n",
    "        metrics_df = metrics.reset_index()\n",
    "        metrics_df.columns = [grouping] + list(metrics_df.columns[1:])\n",
    "        grouped_metrics_tables_xgb[stock] = metrics_df\n",
    "    return grouped_metrics_tables_xgb\n",
    "\n",
    "# Create and Save Grouped Metrics Tables\n",
    "for grouping in ['Month', 'Quarter', 'Season']:\n",
    "    grouped_tables_xgb = create_grouped_metrics_tables_xgb(grouped_metrics_all_xgb, grouping)\n",
    "    for stock, table in grouped_tables_xgb.items():\n",
    "        print(f\"\\n{'='*50}\\n{grouping} Evaluation Metrics for {stock} - XGBoost\\n{'='*50}\")\n",
    "        display(table)\n",
    "        filename = f'{stock}_{grouping}_evaluation_metrics_xgb.csv'\n",
    "        table.to_csv(filename, index=False)\n",
    "        print(f\" - {grouping} Evaluation Metrics table for {stock} saved as '{filename}'.\")\n",
    "    \n",
    "    # Create Comparative Metrics Tables Across Stocks\n",
    "    for metric in ['RMSE', 'MAE', 'R2', 'MAPE']:\n",
    "        comparative_df_xgb = pd.DataFrame({stock: grouped_metrics_all_xgb[grouping][stock][metric] for stock in grouped_metrics_all_xgb[grouping].keys()})\n",
    "        comparative_df_xgb.index.name = grouping\n",
    "        print(f\"\\n{'='*50}\\nComparative {metric} Across {grouping} for All Stocks - XGBoost\\n{'='*50}\")\n",
    "        display(comparative_df_xgb)\n",
    "        filename = f'comparative_{metric}_across_{grouping}_xgb.csv'\n",
    "        comparative_df_xgb.to_csv(filename)\n",
    "        print(f\" - Comparative {metric} Across {grouping} table for XGBoost saved as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa9342-cdf6-4acb-8bcb-d0ce6bcdbd43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Training and Evaluating Random Forest Models\n",
    "\n",
    "# Define Parameters\n",
    "model_save_dir = '../models/random_forest_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "overall_metrics_rf = {}\n",
    "grouped_metrics_all_rf = {'Month': {}, 'Quarter': {}, 'Season': {}}\n",
    "\n",
    "# Function to Add Time Features (if not already added)\n",
    "def add_time_features_rf(eval_df):\n",
    "    eval_df['Month'] = eval_df['Date'].dt.month\n",
    "    eval_df['Quarter'] = eval_df['Date'].dt.quarter\n",
    "    eval_df['Season'] = eval_df['Month'].apply(\n",
    "        lambda month: 'Winter' if month in [12, 1, 2] else\n",
    "                      'Spring' if month in [3, 4, 5] else\n",
    "                      'Summer' if month in [6, 7, 8] else\n",
    "                      'Autumn'\n",
    "    )\n",
    "    return eval_df\n",
    "\n",
    "# Iterate Through Each Stock for Evaluation and Plotting\n",
    "for stock in scaled_daily_data.keys():\n",
    "    print(f\"\\n{'='*50}\\nTraining and Evaluating Random Forest Model for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve Scaled Data\n",
    "    data = scaled_daily_data[stock]\n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Initialize and Train Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train_scaled)\n",
    "    print(f\" - Random Forest model trained for {stock}\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    predictions_scaled = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Inverse Transform Predictions and Targets\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create Evaluation DataFrame\n",
    "    if isinstance(X_test_scaled, pd.DataFrame) and isinstance(X_test_scaled.index, pd.DatetimeIndex):\n",
    "        dates = X_test_scaled.index\n",
    "    elif isinstance(X_test_scaled, pd.DataFrame) and 'Date' in X_test_scaled.columns:\n",
    "        dates = pd.to_datetime(X_test_scaled['Date'])\n",
    "    else:\n",
    "        print(f\" - No Date information found for {stock}. Creating dummy dates.\")\n",
    "        dates = pd.date_range(start='2020-01-01', periods=len(y_test), freq='D')\n",
    "    \n",
    "    eval_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "    \n",
    "    # Add Time Features\n",
    "    eval_df = add_time_features_rf(eval_df)\n",
    "    \n",
    "    # Calculate Overall Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(eval_df['Actual'], eval_df['Predicted']))\n",
    "    mae = mean_absolute_error(eval_df['Actual'], eval_df['Predicted'])\n",
    "    r2 = r2_score(eval_df['Actual'], eval_df['Predicted'])\n",
    "    mape = mean_absolute_percentage_error(eval_df['Actual'], eval_df['Predicted']) * 100\n",
    "    \n",
    "    overall_metrics_rf[stock] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(f\" - Overall Evaluation Metrics for {stock}:\")\n",
    "    print(f\"    RMSE = {rmse:.4f}\")\n",
    "    print(f\"    MAE = {mae:.4f}\")\n",
    "    print(f\"    R2 = {r2:.4f}\")\n",
    "    print(f\"    MAPE = {mape:.2f}%\")\n",
    "    \n",
    "    # Calculate Grouped Metrics\n",
    "    grouped_metrics_month = eval_df.groupby('Month').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_quarter = eval_df.groupby('Quarter').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_season = eval_df.groupby('Season').apply(lambda x: pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['Actual'], x['Predicted'])),\n",
    "        'MAE': mean_absolute_error(x['Actual'], x['Predicted']),\n",
    "        'R2': r2_score(x['Actual'], x['Predicted']),\n",
    "        'MAPE': mean_absolute_percentage_error(x['Actual'], x['Predicted']) * 100\n",
    "    }), include_groups=False)\n",
    "    \n",
    "    grouped_metrics_all_rf['Month'][stock] = grouped_metrics_month\n",
    "    grouped_metrics_all_rf['Quarter'][stock] = grouped_metrics_quarter\n",
    "    grouped_metrics_all_rf['Season'][stock] = grouped_metrics_season\n",
    "    \n",
    "    # Plot Metrics by Season\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'RMSE')\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'MAE')\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'R2')\n",
    "    plot_metrics_by_group(grouped_metrics_season, stock, 'Season', 'MAPE')\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_save_path = os.path.join(model_save_dir, f'rf_{stock}_model.pkl')\n",
    "    joblib.dump(rf_model, model_save_path)\n",
    "    print(f\" - Random Forest model saved for {stock} at {model_save_path}\")\n",
    "    \n",
    "    print(f\"Training and evaluation completed for {stock}.\\n\")\n",
    "\n",
    "# Create Overall Metrics Table\n",
    "overall_metrics_rf_df = pd.DataFrame(overall_metrics_rf).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks - Random Forest\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_rf_df)\n",
    "overall_metrics_rf_df.to_csv('overall_evaluation_metrics_rf.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table for Random Forest saved as 'overall_evaluation_metrics_rf.csv'.\")\n",
    "\n",
    "# Function to Create Grouped Metrics Tables\n",
    "def create_grouped_metrics_tables_rf(grouped_metrics_all_rf, grouping):\n",
    "    grouped_metrics_tables_rf = {}\n",
    "    for stock, metrics in grouped_metrics_all_rf[grouping].items():\n",
    "        metrics_df = metrics.reset_index()\n",
    "        metrics_df.columns = [grouping] + list(metrics_df.columns[1:])\n",
    "        grouped_metrics_tables_rf[stock] = metrics_df\n",
    "    return grouped_metrics_tables_rf\n",
    "\n",
    "# Create and Save Grouped Metrics Tables\n",
    "for grouping in ['Month', 'Quarter', 'Season']:\n",
    "    grouped_tables_rf = create_grouped_metrics_tables_rf(grouped_metrics_all_rf, grouping)\n",
    "    for stock, table in grouped_tables_rf.items():\n",
    "        print(f\"\\n{'='*50}\\n{grouping} Evaluation Metrics for {stock} - Random Forest\\n{'='*50}\")\n",
    "        display(table)\n",
    "        filename = f'{stock}_{grouping}_evaluation_metrics_rf.csv'\n",
    "        table.to_csv(filename, index=False)\n",
    "        print(f\" - {grouping} Evaluation Metrics table for {stock} saved as '{filename}'.\")\n",
    "    \n",
    "    # Create Comparative Metrics Tables Across Stocks\n",
    "    for metric in ['RMSE', 'MAE', 'R2', 'MAPE']:\n",
    "        comparative_df_rf = pd.DataFrame({stock: grouped_metrics_all_rf[grouping][stock][metric] for stock in grouped_metrics_all_rf[grouping].keys()})\n",
    "        comparative_df_rf.index.name = grouping\n",
    "        print(f\"\\n{'='*50}\\nComparative {metric} Across {grouping} for All Stocks - Random Forest\\n{'='*50}\")\n",
    "        display(comparative_df_rf)\n",
    "        filename = f'comparative_{metric}_across_{grouping}_rf.csv'\n",
    "        comparative_df_rf.to_csv(filename)\n",
    "        print(f\" - Comparative {metric} Across {grouping} table for Random Forest saved as '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddb11f-7a92-4407-ad3f-0d806d529537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Developing a Stacking Meta-Model\n",
    "\n",
    "# Define Paths for Models\n",
    "xgb_model_dir = '../models/xgb_models'\n",
    "rf_model_dir = '../models/random_forest_models'\n",
    "lstm_model_dir = '../models/lstm_models'\n",
    "gru_model_dir = '../models/gru_models'\n",
    "meta_model_dir = '../models/meta_model'\n",
    "os.makedirs(meta_model_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Dictionaries to Store Meta-Features and Targets\n",
    "meta_features_train_dict = {}\n",
    "meta_features_test_dict = {}\n",
    "y_train_dict = {}\n",
    "y_test_dict = {}\n",
    "\n",
    "# Function to Create Sequences (Reused from Cell 6)\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i].values)\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Iterate Through Each Stock to Populate Meta-Features\n",
    "for stock in stocks:\n",
    "    print(f\"\\n{'='*50}\\nProcessing Stock: {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # ----- Load Base Models -----\n",
    "    # Load XGBoost Model\n",
    "    xgb_model_path = os.path.join(xgb_model_dir, f'xgb_{stock}_model.json')\n",
    "    if not os.path.exists(xgb_model_path):\n",
    "        print(f\" - XGBoost model for {stock} not found at {xgb_model_path}. Skipping.\")\n",
    "        continue\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "    xgb_model.load_model(xgb_model_path)\n",
    "    print(f\" - Loaded XGBoost model for {stock}.\")\n",
    "    \n",
    "    # Load Random Forest Model\n",
    "    rf_model_path = os.path.join(rf_model_dir, f'rf_{stock}_model.pkl')\n",
    "    if not os.path.exists(rf_model_path):\n",
    "        print(f\" - Random Forest model for {stock} not found at {rf_model_path}. Skipping.\")\n",
    "        continue\n",
    "    rf_model = joblib.load(rf_model_path)\n",
    "    print(f\" - Loaded Random Forest model for {stock}.\")\n",
    "    \n",
    "    # Load LSTM Model\n",
    "    lstm_model_path = os.path.join(lstm_model_dir, f'lstm_{stock}_best.keras')\n",
    "    if not os.path.exists(lstm_model_path):\n",
    "        print(f\" - LSTM model for {stock} not found at {lstm_model_path}. Skipping.\")\n",
    "        continue\n",
    "    lstm_model = load_model(lstm_model_path)\n",
    "    print(f\" - Loaded LSTM model for {stock}.\")\n",
    "    \n",
    "    # Load GRU Model\n",
    "    gru_model_path = os.path.join(gru_model_dir, f'gru_{stock}_best.keras')\n",
    "    if not os.path.exists(gru_model_path):\n",
    "        print(f\" - GRU model for {stock} not found at {gru_model_path}. Skipping.\")\n",
    "        continue\n",
    "    gru_model = load_model(gru_model_path)\n",
    "    print(f\" - Loaded GRU model for {stock}.\")\n",
    "    \n",
    "    # ----- Retrieve Scaled Data -----\n",
    "    data = scaled_daily_data.get(stock)\n",
    "    if data is None:\n",
    "        print(f\" - No scaled data found for {stock}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    X_train_scaled = data['X_train_scaled']\n",
    "    X_test_scaled = data['X_test_scaled']\n",
    "    y_train_scaled = data['y_train_scaled']\n",
    "    y_test_scaled = data['y_test_scaled']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # ----- Create Sequences for LSTM and GRU Models -----\n",
    "    TIMESTEPS = 60  # Ensure consistency\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "    \n",
    "    print(f\" - Training sequences: {X_train_seq.shape}, Training targets: {y_train_seq.shape}\")\n",
    "    print(f\" - Testing sequences: {X_test_seq.shape}, Testing targets: {y_test_seq.shape}\")\n",
    "    \n",
    "    # ----- Generate Predictions from Base Models -----\n",
    "    # XGBoost Predictions\n",
    "    xgb_pred_train_scaled = xgb_model.predict(X_train_scaled.iloc[TIMESTEPS:])\n",
    "    xgb_pred_test_scaled = xgb_model.predict(X_test_scaled.iloc[TIMESTEPS:])\n",
    "    xgb_pred_train = scaler_y.inverse_transform(xgb_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "    xgb_pred_test = scaler_y.inverse_transform(xgb_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Random Forest Predictions\n",
    "    rf_pred_train_scaled = rf_model.predict(X_train_scaled.iloc[TIMESTEPS:])\n",
    "    rf_pred_test_scaled = rf_model.predict(X_test_scaled.iloc[TIMESTEPS:])\n",
    "    rf_pred_train = scaler_y.inverse_transform(rf_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "    rf_pred_test = scaler_y.inverse_transform(rf_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # LSTM Predictions\n",
    "    lstm_pred_train_scaled = lstm_model.predict(X_train_seq).flatten()\n",
    "    lstm_pred_test_scaled = lstm_model.predict(X_test_seq).flatten()\n",
    "    lstm_pred_train = scaler_y.inverse_transform(lstm_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "    lstm_pred_test = scaler_y.inverse_transform(lstm_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # GRU Predictions\n",
    "    gru_pred_train_scaled = gru_model.predict(X_train_seq).flatten()\n",
    "    gru_pred_test_scaled = gru_model.predict(X_test_seq).flatten()\n",
    "    gru_pred_train = scaler_y.inverse_transform(gru_pred_train_scaled.reshape(-1, 1)).flatten()\n",
    "    gru_pred_test = scaler_y.inverse_transform(gru_pred_test_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # ----- Align Predictions -----\n",
    "    min_length_train = min(len(xgb_pred_train), len(rf_pred_train), len(lstm_pred_train), len(gru_pred_train))\n",
    "    min_length_test = min(len(xgb_pred_test), len(rf_pred_test), len(lstm_pred_test), len(gru_pred_test))\n",
    "    \n",
    "    xgb_pred_train = xgb_pred_train[:min_length_train]\n",
    "    rf_pred_train = rf_pred_train[:min_length_train]\n",
    "    lstm_pred_train = lstm_pred_train[:min_length_train]\n",
    "    gru_pred_train = gru_pred_train[:min_length_train]\n",
    "    y_train = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()[:min_length_train]\n",
    "    \n",
    "    xgb_pred_test = xgb_pred_test[:min_length_test]\n",
    "    rf_pred_test = rf_pred_test[:min_length_test]\n",
    "    lstm_pred_test = lstm_pred_test[:min_length_test]\n",
    "    gru_pred_test = gru_pred_test[:min_length_test]\n",
    "    y_test = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()[:min_length_test]\n",
    "    \n",
    "    # ----- Populate Meta-Features for Training Data -----\n",
    "    meta_features_train = pd.DataFrame({\n",
    "        'XGB_Pred': xgb_pred_train,\n",
    "        'RF_Pred': rf_pred_train,\n",
    "        'LSTM_Pred': lstm_pred_train,\n",
    "        'GRU_Pred': gru_pred_train\n",
    "    })\n",
    "    meta_features_train_dict[stock] = meta_features_train\n",
    "    print(f\" - Meta-features for training data populated for {stock}.\")\n",
    "    \n",
    "    # ----- Populate Meta-Features for Test Data -----\n",
    "    meta_features_test = pd.DataFrame({\n",
    "        'XGB_Pred': xgb_pred_test,\n",
    "        'RF_Pred': rf_pred_test,\n",
    "        'LSTM_Pred': lstm_pred_test,\n",
    "        'GRU_Pred': gru_pred_test\n",
    "    })\n",
    "    meta_features_test_dict[stock] = meta_features_test\n",
    "    print(f\" - Meta-features for testing data populated for {stock}.\")\n",
    "    \n",
    "    # ----- Store Target Variables -----\n",
    "    y_train_dict[stock] = y_train\n",
    "    y_test_dict[stock] = y_test\n",
    "    print(f\" - Target variables stored for {stock}.\")\n",
    "\n",
    "# ----- Check Meta-Features DataFrames -----\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nMeta-Features DataFrame Shapes\\n\" + \"=\"*50)\n",
    "for stock in meta_features_train_dict.keys():\n",
    "    print(f\" - {stock}: meta_features_train shape: {meta_features_train_dict[stock].shape}, meta_features_test shape: {meta_features_test_dict[stock].shape}, y_train shape: {y_train_dict[stock].shape}, y_test shape: {y_test_dict[stock].shape}\")\n",
    "\n",
    "# ----- Train Meta-Models for Each Stock -----\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nTraining Meta-Models for Each Stock\\n\" + \"=\"*50)\n",
    "\n",
    "meta_model_per_stock = {}\n",
    "\n",
    "for stock in meta_features_train_dict.keys():\n",
    "    print(f\"\\nTraining Meta-Model for {stock}\")\n",
    "    \n",
    "    meta_features_train = meta_features_train_dict[stock]\n",
    "    meta_features_test = meta_features_test_dict[stock]\n",
    "    y_train = y_train_dict[stock]\n",
    "    y_test = y_test_dict[stock]\n",
    "    \n",
    "    # Check if meta_features_train and y_train are non-empty\n",
    "    if meta_features_train.empty or len(y_train) == 0:\n",
    "        print(f\" - Empty meta-features or target variables for {stock}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Train Meta-Model (Ridge Regression with Cross-Validation)\n",
    "    meta_model = RidgeCV()\n",
    "    meta_model.fit(meta_features_train, y_train)\n",
    "    print(f\" - Meta-Model (Ridge Regression) trained successfully for {stock}.\")\n",
    "    \n",
    "    # Save Meta-Model\n",
    "    meta_model_path = os.path.join(meta_model_dir, f'stacking_meta_model_{stock}.pkl')\n",
    "    joblib.dump(meta_model, meta_model_path)\n",
    "    print(f\" - Meta-Model saved at '{meta_model_path}'\")\n",
    "    \n",
    "    # ----- Generate Meta-Predictions on Test Data -----\n",
    "    meta_pred_test = meta_model.predict(meta_features_test)\n",
    "    \n",
    "    # ----- Evaluate Meta-Model -----\n",
    "    rmse_meta = np.sqrt(mean_squared_error(y_test, meta_pred_test))\n",
    "    mae_meta = mean_absolute_error(y_test, meta_pred_test)\n",
    "    r2_meta = r2_score(y_test, meta_pred_test)\n",
    "    mape_meta = mean_absolute_percentage_error(y_test, meta_pred_test) * 100\n",
    "    \n",
    "    print(f\"\\nMeta-Model Evaluation Metrics for {stock}:\")\n",
    "    print(f\"    RMSE = {rmse_meta:.4f}\")\n",
    "    print(f\"    MAE = {mae_meta:.4f}\")\n",
    "    print(f\"    R2 = {r2_meta:.4f}\")\n",
    "    print(f\"    MAPE = {mape_meta:.2f}%\")\n",
    "    \n",
    "    # ----- Create Evaluation DataFrame for Meta-Model -----\n",
    "    # Assuming 'Date' is aligned with the test data's index\n",
    "    X_test_scaled = scaled_daily_data[stock]['X_test_scaled']\n",
    "    if isinstance(X_test_scaled, pd.DataFrame) and isinstance(X_test_scaled.index, pd.DatetimeIndex):\n",
    "        dates = X_test_scaled.index\n",
    "    elif isinstance(X_test_scaled, pd.DataFrame) and 'Date' in X_test_scaled.columns:\n",
    "        dates = pd.to_datetime(X_test_scaled['Date'], errors='coerce')\n",
    "    else:\n",
    "        print(f\" - No Date information found for {stock}. Creating dummy dates.\")\n",
    "        if len(meta_pred_test) > 0:\n",
    "            last_date = X_test_scaled.index[-1] if isinstance(X_test_scaled.index, pd.DatetimeIndex) else pd.Timestamp('2020-01-01')\n",
    "            dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=len(meta_pred_test), freq='D')\n",
    "        else:\n",
    "            dates = pd.date_range(start='2020-01-01', periods=len(meta_pred_test), freq='D')\n",
    "    \n",
    "    eval_df_meta = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_test,\n",
    "        'Meta_Predicted': meta_pred_test\n",
    "    })\n",
    "    \n",
    "    # ----- Add Time Features -----\n",
    "    def add_time_features_meta(eval_df):\n",
    "        # Ensure 'Date' is in datetime format\n",
    "        if 'Date' not in eval_df.columns:\n",
    "            raise KeyError(\"The DataFrame does not contain a 'Date' column.\")\n",
    "        \n",
    "        if not pd.api.types.is_datetime64_any_dtype(eval_df['Date']):\n",
    "            eval_df['Date'] = pd.to_datetime(eval_df['Date'], errors='coerce')\n",
    "        \n",
    "        # Check for any NaT values after conversion\n",
    "        if eval_df['Date'].isnull().any():\n",
    "            print(\"Warning: Some 'Date' entries could not be converted to datetime and will be dropped.\")\n",
    "            eval_df = eval_df.dropna(subset=['Date'])\n",
    "        \n",
    "        eval_df['Month'] = eval_df['Date'].dt.month\n",
    "        eval_df['Quarter'] = eval_df['Date'].dt.quarter\n",
    "        eval_df['Season'] = eval_df['Month'].apply(\n",
    "            lambda month: 'Winter' if month in [12, 1, 2] else\n",
    "                          'Spring' if month in [3, 4, 5] else\n",
    "                          'Summer' if month in [6, 7, 8] else\n",
    "                          'Autumn'\n",
    "        )\n",
    "        return eval_df\n",
    "    \n",
    "    try:\n",
    "        eval_df_meta = add_time_features_meta(eval_df_meta)\n",
    "        print(f\" - Time features added to the meta-model evaluation DataFrame for {stock}.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # ----- Save Meta-Model Predictions (Optional) -----\n",
    "    meta_pred_save_path = os.path.join(meta_model_dir, f'meta_predictions_{stock}.csv')\n",
    "    eval_df_meta.to_csv(meta_pred_save_path, index=False)\n",
    "    print(f\" - Meta-Model predictions saved at '{meta_pred_save_path}'\")\n",
    "    \n",
    "    # ----- Store Meta-Model Performance -----\n",
    "    meta_model_per_stock[stock] = {\n",
    "        'RMSE': rmse_meta,\n",
    "        'MAE': mae_meta,\n",
    "        'R2': r2_meta,\n",
    "        'MAPE': mape_meta\n",
    "    }\n",
    "\n",
    "# ----- Create Overall Metrics Table -----\n",
    "overall_metrics_meta_df = pd.DataFrame(meta_model_per_stock).T\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Evaluation Metrics for All Stocks - Meta Stacked Model\")\n",
    "print(\"=\"*50)\n",
    "display(overall_metrics_meta_df)\n",
    "overall_metrics_meta_df.to_csv('overall_evaluation_metrics_meta_stacked.csv')\n",
    "print(\"\\n - Overall Evaluation Metrics table for Meta Stacked Model saved as 'overall_evaluation_metrics_meta_stacked.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1493d1-a997-4702-9859-9b3b9a9bbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12A: Define Essential Functions\n",
    "\n",
    "# Suppress specific warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def load_models(stock):\n",
    "    \"\"\"\n",
    "    Load base models and meta-model for a given stock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define model directories\n",
    "        xgb_model_dir = '../models/xgb_models'\n",
    "        rf_model_dir = '../models/random_forest_models'\n",
    "        lstm_model_dir = '../models/lstm_models'\n",
    "        gru_model_dir = '../models/gru_models'\n",
    "        meta_model_dir = '../models/meta_model'\n",
    "        scalers_dir = '../models/scalers'\n",
    "        \n",
    "        # Load XGBoost Model\n",
    "        xgb_model_path = os.path.join(xgb_model_dir, f'xgb_{stock}_model.json')\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.load_model(xgb_model_path)\n",
    "        print(f\" - Loaded XGBoost model from '{xgb_model_path}'\")\n",
    "        \n",
    "        # Load Random Forest Model\n",
    "        rf_model_path = os.path.join(rf_model_dir, f'rf_{stock}_model.pkl')\n",
    "        rf_model = joblib.load(rf_model_path)\n",
    "        print(f\" - Loaded Random Forest model from '{rf_model_path}'\")\n",
    "        \n",
    "        # Load LSTM Model\n",
    "        lstm_model_path = os.path.join(lstm_model_dir, f'lstm_{stock}_best.keras')\n",
    "        lstm_model = load_model(lstm_model_path)\n",
    "        print(f\" - Loaded LSTM model from '{lstm_model_path}'\")\n",
    "        \n",
    "        # Load GRU Model\n",
    "        gru_model_path = os.path.join(gru_model_dir, f'gru_{stock}_best.keras')\n",
    "        gru_model = load_model(gru_model_path)\n",
    "        print(f\" - Loaded GRU model from '{gru_model_path}'\")\n",
    "        \n",
    "        # Load Meta-Model\n",
    "        meta_model_path = os.path.join(meta_model_dir, f'stacking_meta_model_{stock}.pkl')\n",
    "        meta_model = joblib.load(meta_model_path)\n",
    "        print(f\" - Loaded Meta-Model from '{meta_model_path}'\")\n",
    "        \n",
    "        # Load Scalers\n",
    "        scaler_X_path = os.path.join(scalers_dir, f'minmax_scaler_X_{stock}.joblib')\n",
    "        scaler_y_path = os.path.join(scalers_dir, f'minmax_scaler_y_{stock}.joblib')\n",
    "        scaler_X = joblib.load(scaler_X_path)\n",
    "        scaler_y = joblib.load(scaler_y_path)\n",
    "        print(f\" - Loaded Scalers from '{scaler_X_path}' and '{scaler_y_path}'\")\n",
    "        \n",
    "        return xgb_model, rf_model, lstm_model, gru_model, meta_model, scaler_X, scaler_y\n",
    "    except Exception as e:\n",
    "        print(f\" - Error loading models or scalers for {stock}: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_vwap(df):\n",
    "    \"\"\"Calculate Volume Weighted Average Price (VWAP).\"\"\"\n",
    "    typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    vwap = (typical_price * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return vwap\n",
    "\n",
    "def add_basic_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Add basic technical indicators to the DataFrame.\n",
    "    \"\"\"\n",
    "    # Simple and Exponential Moving Averages\n",
    "    df['SMA_20'] = ta.trend.SMAIndicator(close=df['Close'], window=20, fillna=False).sma_indicator()\n",
    "    df['EMA_20'] = ta.trend.EMAIndicator(close=df['Close'], window=20, fillna=False).ema_indicator()\n",
    "\n",
    "    # Relative Strength Index\n",
    "    df['RSI_14'] = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False).rsi()\n",
    "\n",
    "    # Moving Average Convergence Divergence\n",
    "    macd = ta.trend.MACD(close=df['Close'], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_signal'] = macd.macd_signal()\n",
    "    df['MACD_diff'] = macd.macd_diff()\n",
    "\n",
    "    # Average True Range\n",
    "    df['ATR_14'] = ta.volatility.AverageTrueRange(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], window=14, fillna=False\n",
    "    ).average_true_range()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)\n",
    "    df['Bollinger_High'] = bollinger.bollinger_hband()\n",
    "    df['Bollinger_Low'] = bollinger.bollinger_lband()\n",
    "    df['Bollinger_Width'] = bollinger.bollinger_wband()\n",
    "    \n",
    "    # On-Balance Volume\n",
    "    df['OBV'] = ta.volume.OnBalanceVolumeIndicator(close=df['Close'], volume=df['Volume'], fillna=False).on_balance_volume()\n",
    "    \n",
    "    # Stochastic Oscillator\n",
    "    stoch = ta.momentum.StochasticOscillator(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], window=14, smooth_window=3, fillna=False\n",
    "    )\n",
    "    df['Stoch_%K'] = stoch.stoch()\n",
    "    df['Stoch_%D'] = stoch.stoch_signal()\n",
    "\n",
    "    # Rate of Change\n",
    "    df['ROC_10'] = ta.momentum.ROCIndicator(close=df['Close'], window=10, fillna=False).roc()\n",
    "    \n",
    "    # Daily Returns\n",
    "    df['Daily_Return'] = df['Close'].pct_change()\n",
    "    \n",
    "    # Close-Open Percent\n",
    "    df['Close_Open_Percent'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    \n",
    "    # High-Low Percent\n",
    "    df['High_Low_Percent'] = (df['High'] - df['Low']) / df['Low'] * 100\n",
    "\n",
    "    # Fill any NaN values\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_advanced_features_daily(df):\n",
    "    \"\"\"\n",
    "    Add advanced features to the DataFrame.\n",
    "    \"\"\"\n",
    "    # a. Relative Strength Index (RSI_14)\n",
    "    # Already added in basic indicators; skip if duplicate\n",
    "\n",
    "    # b. Moving Average Convergence Divergence (MACD)\n",
    "    # Already added in basic indicators; skip if duplicate\n",
    "\n",
    "    # c. Average True Range (ATR_14)\n",
    "    # Already added in basic indicators; skip if duplicate\n",
    "\n",
    "    # d. Bollinger Bands Width (Bollinger_Width)\n",
    "    # Already added in basic indicators; skip if duplicate\n",
    "\n",
    "    # e. Keltner Channels\n",
    "    keltner = ta.volatility.KeltnerChannel(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], window=20, window_atr=10, fillna=False\n",
    "    )\n",
    "    df['Keltner_High'] = keltner.keltner_channel_hband()\n",
    "    df['Keltner_Low'] = keltner.keltner_channel_lband()\n",
    "    df['Keltner_Width'] = df['Keltner_High'] - df['Keltner_Low']\n",
    "\n",
    "    # f. Chaikin Money Flow (CMF)\n",
    "    cmf_indicator = ta.volume.ChaikinMoneyFlowIndicator(\n",
    "        high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume'], window=20, fillna=False\n",
    "    )\n",
    "    df['CMF'] = cmf_indicator.chaikin_money_flow()\n",
    "\n",
    "    # 2. Lag Features\n",
    "    for lag in range(1, 4):\n",
    "        df[f'Lag_Close_{lag}'] = df['Close'].shift(lag)\n",
    "\n",
    "    # 3. Moving Averages and Standard Deviations\n",
    "    df['MA_5'] = df['Close'].rolling(window=5, min_periods=5).mean()\n",
    "    df['MA_Volume_10'] = df['Volume'].rolling(window=10, min_periods=10).mean()\n",
    "    df['STD_Close_5'] = df['Close'].rolling(window=5, min_periods=5).std()\n",
    "    df['MA_RSI_10'] = df['RSI_14'].rolling(window=10, min_periods=10).mean()\n",
    "    df['MA_MACD_10'] = df['MACD'].rolling(window=10, min_periods=10).mean()\n",
    "    df['MA_ATR_10'] = df['ATR_14'].rolling(window=10, min_periods=10).mean()\n",
    "    df['MA_Bollinger_Width_10'] = df['Bollinger_Width'].rolling(window=10, min_periods=10).mean()\n",
    "\n",
    "    # 4. VWAP and its Moving Average\n",
    "    df['VWAP'] = calculate_vwap(df)\n",
    "    df['MA_VWAP_10'] = df['VWAP'].rolling(window=10, min_periods=10).mean()\n",
    "\n",
    "    # 5. Date Features\n",
    "    if 'Date' not in df.columns:\n",
    "        raise KeyError(\"Missing 'Date' column required for date-based feature engineering.\")\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "\n",
    "    # 6. Rolling Correlations and Momentum\n",
    "    df['Rolling_Corr_Close_Volume_10'] = df['Close'].rolling(window=10, min_periods=10).corr(df['Volume'])\n",
    "    df['Momentum_5'] = df['Close'].diff(5)\n",
    "    df['Volatility_10'] = df['Close'].rolling(window=10, min_periods=10).std()\n",
    "\n",
    "    # 7. Cyclical Encoding for Day of Week and Month\n",
    "    df['Day_of_Week_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "    df['Day_of_Week_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    # Drop original Day_of_Week and Month\n",
    "    df.drop(['Day_of_Week', 'Month'], axis=1, inplace=True)\n",
    "\n",
    "    # 8. Interaction Features\n",
    "    df['MACD_diff_RSI_14'] = df['MACD_diff'] * df['RSI_14']\n",
    "    df['MACD_diff_ATR_14'] = df['MACD_diff'] * df['ATR_14']\n",
    "    df['RSI_14_ATR_14'] = df['RSI_14'] * df['ATR_14']\n",
    "\n",
    "    # 9. Trend and Seasonal Components\n",
    "    df['Trend'] = df['Close'].rolling(window=30, min_periods=30).mean()\n",
    "    df['Seasonal'] = df['Close'] - df['Trend']\n",
    "\n",
    "    # 10. Rolling Skewness and Kurtosis\n",
    "    df['Rolling_Skew_Close_10'] = df['Close'].rolling(window=10, min_periods=10).skew()\n",
    "    df['Rolling_Kurt_Close_10'] = df['Close'].rolling(window=10, min_periods=10).kurt()\n",
    "\n",
    "    # Fill any remaining NaN values\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_next_day_features(current_data, scaler_X, timesteps=60):\n",
    "    \"\"\"\n",
    "    Generate features for the next day forecast.\n",
    "    \"\"\"\n",
    "    if len(current_data) < timesteps:\n",
    "        raise ValueError(\"Insufficient data to generate features.\")\n",
    "    \n",
    "    latest_data = current_data.copy()\n",
    "    if 'Close' not in latest_data.columns:\n",
    "        raise KeyError(\"'Close' column is missing in current_data.\")\n",
    "    \n",
    "    # Apply basic and advanced feature engineering\n",
    "    latest_data = add_basic_technical_indicators(latest_data)\n",
    "    latest_data = add_advanced_features_daily(latest_data)\n",
    "    \n",
    "    # Drop 'Close' and 'Date' to get feature columns\n",
    "    if 'Date' in latest_data.columns:\n",
    "        features_for_models = latest_data.drop(columns=['Close', 'Date'])\n",
    "    else:\n",
    "        features_for_models = latest_data.drop(columns=['Close'])\n",
    "    \n",
    "    # Ensure features are in the same order as during training\n",
    "    expected_features = scaler_X.feature_names_in_\n",
    "    features_for_models = features_for_models[expected_features]\n",
    "    \n",
    "    # Verify feature count matches scaler's expectation\n",
    "    actual_features = features_for_models.shape[1]\n",
    "    if actual_features != len(expected_features):\n",
    "        raise ValueError(f\"Feature count mismatch: expected {len(expected_features)}, got {actual_features}\")\n",
    "    \n",
    "    # For XGBoost and Random Forest: use the latest timestep's features\n",
    "    latest_timestep = features_for_models.iloc[-1:].copy()  # DataFrame with 1 row\n",
    "    xgb_rf_features_scaled = scaler_X.transform(latest_timestep)\n",
    "    \n",
    "    # For LSTM and GRU: use the entire sequence of timesteps\n",
    "    lstm_gru_features = features_for_models.tail(timesteps).copy()\n",
    "    lstm_gru_features_scaled = scaler_X.transform(lstm_gru_features).reshape(1, timesteps, -1)\n",
    "    \n",
    "    # Return the scaled features and the latest features for appending\n",
    "    latest_features = latest_timestep.iloc[0].copy()\n",
    "    return xgb_rf_features_scaled, lstm_gru_features_scaled, latest_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae03e0-1934-44d3-b2ca-19136e0116ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12B: Initialize and Prepare Data\n",
    "\n",
    "# Define Paths\n",
    "raw_data_dir = '../data/stock_data'  # Directory containing raw CSVs\n",
    "scaled_data_dir = '../models/scalers'  # Directory containing scalers\n",
    "forecast_save_dir = '../models/future_forecasts'\n",
    "os.makedirs(forecast_save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a dictionary to store models and scaled data for each stock\n",
    "models_per_stock = {}\n",
    "timesteps = 60  # Number of timesteps used in LSTM/GRU models\n",
    "\n",
    "for stock in stocks:\n",
    "    print(f\"\\n{'='*50}\\nProcessing Stock: {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models(stock)\n",
    "    if models is None:\n",
    "        print(f\" - Skipping stock '{stock}' due to model loading issues.\")\n",
    "        continue\n",
    "    xgb_model, rf_model, lstm_model, gru_model, meta_model, scaler_X, scaler_y = models\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_csv_path = os.path.join(raw_data_dir, f\"{stock}_daily.csv\")\n",
    "    if not os.path.exists(raw_csv_path):\n",
    "        print(f\" - Raw data CSV not found at '{raw_csv_path}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(raw_csv_path)\n",
    "        print(f\" - Loaded raw data from '{raw_csv_path}'. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error reading CSV for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Ensure 'Date' column is present\n",
    "    if 'Date' not in df.columns:\n",
    "        print(f\" - 'Date' column missing in '{raw_csv_path}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert 'Date' to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isnull().any():\n",
    "        print(f\" - Some 'Date' entries could not be converted to datetime for {stock}. Dropping these rows.\")\n",
    "        df.dropna(subset=['Date'], inplace=True)\n",
    "    \n",
    "    # Sort by Date\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Add Basic Technical Indicators\n",
    "    try:\n",
    "        df = add_basic_technical_indicators(df)\n",
    "        print(f\" - Added basic technical indicators. Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error during adding basic technical indicators for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Add Advanced Features\n",
    "    try:\n",
    "        df_fe = add_advanced_features_daily(df)\n",
    "        print(f\" - Applied advanced feature engineering. Shape: {df_fe.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error during advanced feature engineering for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Keep only necessary recent data\n",
    "    current_data = df_fe.copy()\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_cols = [col for col in current_data.columns if col not in ['Date', 'Close']]\n",
    "    X_current = current_data[feature_cols].copy()\n",
    "    \n",
    "    # Ensure features are in the same order as during training\n",
    "    expected_features = scaler_X.feature_names_in_\n",
    "    missing_features = [feat for feat in expected_features if feat not in X_current.columns]\n",
    "    if missing_features:\n",
    "        print(f\" - Missing features in current data for {stock}: {missing_features}\")\n",
    "        continue\n",
    "    X_current = X_current[expected_features]\n",
    "    \n",
    "    # Scale features and target\n",
    "    try:\n",
    "        X_current_scaled = pd.DataFrame(scaler_X.transform(X_current), columns=X_current.columns)\n",
    "        current_data_scaled = X_current_scaled.copy()\n",
    "        current_data_scaled['Close'] = scaler_y.transform(current_data['Close'].values.reshape(-1, 1)).flatten()\n",
    "        current_data_scaled['Date'] = current_data['Date'].values\n",
    "        print(f\" - Scaled current data for {stock}\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Error scaling data for {stock}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Store prepared data in models_per_stock\n",
    "    models_per_stock[stock] = {\n",
    "        'models': models,\n",
    "        'current_data': current_data_scaled.tail(timesteps).copy(),  # Keep the last 'timesteps' rows\n",
    "        'scaler_y': scaler_y\n",
    "    }\n",
    "    \n",
    "    print(f\" - Current data for {stock} loaded and prepared. Total samples: {len(current_data_scaled)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825e712-a801-4c36-9688-01848a88f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13A: Forecasting Setup and Initialization\n",
    "\n",
    "# Import necessary libraries (if not already imported)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Ensure the forecast directory exists\n",
    "forecast_save_dir = '../models/future_forecasts'\n",
    "os.makedirs(forecast_save_dir, exist_ok=True)\n",
    "\n",
    "# Forecast parameters\n",
    "forecast_days = 30  # Number of days to forecast\n",
    "\n",
    "# Initialize a dictionary to store forecast results for each stock\n",
    "forecast_results = {stock: {} for stock in stocks}\n",
    "\n",
    "# Function to check for NaN or Inf values in DataFrame\n",
    "def check_nan_inf(df, day, stock):\n",
    "    if df.isnull().values.any():\n",
    "        print(f\"   - Warning: NaN values found in features on Day {day} for {stock}\")\n",
    "        print(df.isnull().sum())\n",
    "        return True\n",
    "    if np.isinf(df.values).any():\n",
    "        print(f\"   - Warning: Inf values found in features on Day {day} for {stock}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(\"Forecasting setup completed. Ready to proceed to forecasting in Cell 13B.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed971a0b-f6b9-4161-aca7-a8ae0b89a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13B: Perform Recursive Forecasting and Display Results\n",
    "\n",
    "for stock in stocks:\n",
    "    print(f\"\\n{'='*50}\\nStarting Forecasting for {stock}\\n{'='*50}\")\n",
    "    \n",
    "    # Retrieve models and current_data from models_per_stock\n",
    "    stock_data = models_per_stock.get(stock)\n",
    "    if stock_data is None:\n",
    "        print(f\" - No prepared data for '{stock}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    models = stock_data['models']\n",
    "    current_data = stock_data['current_data'].copy()\n",
    "    scaler_y = stock_data['scaler_y']\n",
    "    \n",
    "    xgb_model, rf_model, lstm_model, gru_model, meta_model, scaler_X, scaler_y = models\n",
    "    \n",
    "    # Initialize the last date\n",
    "    last_date = current_data['Date'].iloc[-1]\n",
    "    print(f\" - Last historical date: {last_date}\")\n",
    "    \n",
    "    # Initialize lists to store individual model predictions and dates\n",
    "    xgb_predictions = []\n",
    "    rf_predictions = []\n",
    "    lstm_predictions = []\n",
    "    gru_predictions = []\n",
    "    meta_predictions = []\n",
    "    forecasted_dates = []\n",
    "    \n",
    "    for day in range(1, forecast_days + 1):\n",
    "        try:\n",
    "            # Generate features for the next day\n",
    "            xgb_rf_scaled, lstm_gru_scaled, latest_features = generate_next_day_features(current_data, scaler_X)\n",
    "            \n",
    "            # Predictions\n",
    "            xgb_pred_scaled = xgb_model.predict(xgb_rf_scaled)\n",
    "            xgb_pred = scaler_y.inverse_transform(xgb_pred_scaled.reshape(-1, 1)).flatten()[0]\n",
    "            \n",
    "            rf_pred_scaled = rf_model.predict(xgb_rf_scaled)\n",
    "            rf_pred = scaler_y.inverse_transform(rf_pred_scaled.reshape(-1, 1)).flatten()[0]\n",
    "            \n",
    "            lstm_pred_scaled = lstm_model.predict(lstm_gru_scaled)\n",
    "            lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled.reshape(-1, 1)).flatten()[0]\n",
    "            \n",
    "            gru_pred_scaled = gru_model.predict(lstm_gru_scaled)\n",
    "            gru_pred = scaler_y.inverse_transform(gru_pred_scaled.reshape(-1, 1)).flatten()[0]\n",
    "            \n",
    "            # Meta-Model Prediction\n",
    "            meta_features = pd.DataFrame({\n",
    "                'XGB_Pred': [xgb_pred],\n",
    "                'RF_Pred': [rf_pred],\n",
    "                'LSTM_Pred': [lstm_pred],\n",
    "                'GRU_Pred': [gru_pred]\n",
    "            })\n",
    "            meta_pred = meta_model.predict(meta_features)[0]\n",
    "            \n",
    "            # Append predictions and dates\n",
    "            xgb_predictions.append(xgb_pred)\n",
    "            rf_predictions.append(rf_pred)\n",
    "            lstm_predictions.append(lstm_pred)\n",
    "            gru_predictions.append(gru_pred)\n",
    "            meta_predictions.append(meta_pred)\n",
    "            forecasted_dates.append(last_date + pd.Timedelta(days=1))\n",
    "            \n",
    "            print(f\"   - Day {day}: Meta-Model Predicted Price = {meta_pred:.2f}\")\n",
    "            \n",
    "            # Append the forecasted 'Close' and 'Date' to current_data\n",
    "            new_close_scaled = scaler_y.transform([[meta_pred]])[0][0]  # Transform to scaled value\n",
    "            new_date = last_date + pd.Timedelta(days=1)\n",
    "            \n",
    "            # Create a new row with updated 'Close' and 'Date'\n",
    "            new_row = latest_features.copy()\n",
    "            new_row['Close'] = new_close_scaled\n",
    "            new_row['Date'] = new_date\n",
    "            \n",
    "            # Convert Series to DataFrame\n",
    "            new_row_df = new_row.to_frame().T\n",
    "            \n",
    "            # Concatenate the new row to current_data\n",
    "            current_data = pd.concat([current_data, new_row_df], ignore_index=True)\n",
    "            \n",
    "            # Update last_date\n",
    "            last_date = new_date\n",
    "            \n",
    "            # Keep only the last 'timesteps' rows\n",
    "            if len(current_data) > timesteps:\n",
    "                current_data = current_data.iloc[-timesteps:].copy()\n",
    "            \n",
    "            # Recompute features for current_data after adding the new row\n",
    "            # Inverse transform 'Close' back to original scale for feature computation\n",
    "            current_data['Close'] = scaler_y.inverse_transform(current_data['Close'].values.reshape(-1, 1)).flatten()\n",
    "            # Ensure 'Date' is in datetime format\n",
    "            current_data['Date'] = pd.to_datetime(current_data['Date'])\n",
    "            \n",
    "            # Recompute features\n",
    "            current_data_with_features = current_data.copy()\n",
    "            current_data_with_features = add_basic_technical_indicators(current_data_with_features)\n",
    "            current_data_with_features = add_advanced_features_daily(current_data_with_features)\n",
    "            \n",
    "            # **Debugging: Print data types and sample data**\n",
    "            print(f\"   - Data types of features on Day {day} for {stock}:\\n{current_data_with_features.dtypes}\")\n",
    "            print(f\"   - Sample data on Day {day} for {stock}:\\n{current_data_with_features.tail(3)}\")\n",
    "            \n",
    "            # Ensure numeric data types\n",
    "            numeric_cols = current_data_with_features.select_dtypes(include=[np.number]).columns\n",
    "            current_data_with_features[numeric_cols] = current_data_with_features[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "            \n",
    "            # Handle potential NaN or Inf values\n",
    "            current_data_with_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            current_data_with_features.fillna(method='bfill', inplace=True)\n",
    "            current_data_with_features.fillna(method='ffill', inplace=True)\n",
    "            \n",
    "            # Check for NaN or Inf values\n",
    "            if check_nan_inf(current_data_with_features, day, stock):\n",
    "                break  # Exit the loop if invalid values are found\n",
    "            \n",
    "            # Scale features (excluding 'Close' and 'Date')\n",
    "            feature_cols = [col for col in current_data_with_features.columns if col not in ['Date', 'Close']]\n",
    "            X_current_scaled = pd.DataFrame(scaler_X.transform(current_data_with_features[feature_cols]), columns=feature_cols)\n",
    "            \n",
    "            # Update current_data with scaled features, retaining 'Close' and 'Date'\n",
    "            current_data = X_current_scaled.copy()\n",
    "            current_data['Close'] = scaler_y.transform(current_data_with_features['Close'].values.reshape(-1, 1)).flatten()\n",
    "            current_data['Date'] = current_data_with_features['Date'].values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" - Error forecasting Day {day} for {stock}: {e}\")\n",
    "            break\n",
    "    \n",
    "    if not meta_predictions:\n",
    "        print(f\" - No forecasted results for {stock}. Skipping DataFrame creation.\")\n",
    "        continue\n",
    "    \n",
    "    # Create Forecast DataFrame with all model predictions\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'Date': forecasted_dates,\n",
    "        'XGB_Pred': xgb_predictions,\n",
    "        'RF_Pred': rf_predictions,\n",
    "        'LSTM_Pred': lstm_predictions,\n",
    "        'GRU_Pred': gru_predictions,\n",
    "        'Meta_Pred': meta_predictions\n",
    "    })\n",
    "    \n",
    "    # Display the forecasted results\n",
    "    print(f\"\\nForecasted Prices for {stock} - Next {len(forecast_df)} Days:\")\n",
    "    display(forecast_df)\n",
    "    \n",
    "    # Calculate and display summary statistics\n",
    "    forecast_stats = forecast_df[['XGB_Pred', 'RF_Pred', 'LSTM_Pred', 'GRU_Pred', 'Meta_Pred']].describe()\n",
    "    print(f\"\\nSummary Statistics for Forecasted Prices of {stock}:\")\n",
    "    display(forecast_stats)\n",
    "    \n",
    "    # Save the forecast to CSV\n",
    "    forecast_save_path = os.path.join(forecast_save_dir, f'future_forecasts_{stock}.csv')\n",
    "    forecast_df.to_csv(forecast_save_path, index=False)\n",
    "    print(f\" - Future forecasts saved at '{forecast_save_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873b9a7-9722-424b-b6b8-cf547e85ecf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
